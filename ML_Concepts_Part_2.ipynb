{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML  -Concepts - Part 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K9N_RI9Ww7l",
        "colab_type": "text"
      },
      "source": [
        "[NN with scikit](#nsk)\n",
        "\n",
        "[Naivebayes](#nb)\n",
        "\n",
        "[Decision trees](#dt)\n",
        "\n",
        "[Random Forest](#rf)\n",
        "\n",
        "[Boosting](#boo)\n",
        "\n",
        "[PCA-LDA-GMM](#pca)\n",
        "\n",
        "[Intro to Tensorflow](#tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66KJP4KWtpy",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"nsk\"></a>\n",
        "\n",
        "### Neural Networks with scikit\n",
        "\n",
        "### Perceptron Class\n",
        "\n",
        "### We will start with the Perceptron class contained in Scikit-Learn. We will use it on the iris dataset, which we had already used in our chapter on k-nearest neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q0ljzRgWgUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aa220d16-79ef-419f-d8dc-6722b54de0f7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "print(iris.data[:3])\n",
        "print(iris.data[15:18])\n",
        "print(iris.data[37:40])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]]\n",
            "[[5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]]\n",
            "[[4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_ltw8uXWgW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we extract only the lengths and widths of the petals:\n",
        "\n",
        "X = iris.data[:, (2, 3)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVxwjNURW-fj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8848882-74f0-45b0-bdb4-03d206b6dc3a"
      },
      "source": [
        "# iris.label contains the labels 0, 1 and 2 corresponding three species of Iris flower:\n",
        "\n",
        "#Iris setosa,\n",
        "#Iris virginica and\n",
        "#Iris versicolor.\n",
        "\n",
        "print(iris.target)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5dZWEDQW-iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e8d7d386-686a-4fd4-8508-36769afd4791"
      },
      "source": [
        "#  We turn the three classes into two classes, i.e.\n",
        "\n",
        "#Iris setosa\n",
        "#not Iris setosa (this means Iris virginica or Iris versicolor)\n",
        "\n",
        "y = (iris.target==0).astype(np.int8)\n",
        "print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PapGSZ2aW-lA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cf15582b-662f-44fa-d5f6-4e684c42ce3c"
      },
      "source": [
        "#  We create now a Perceptron and fit the data X and y:\n",
        "\n",
        "p = Perceptron(random_state=42,\n",
        "              max_iter=10,\n",
        "              tol=0.001)\n",
        "p.fit(X, y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=10, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty=None, random_state=42, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkKhiEfrXT3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a5c7965-63c1-4ad4-8db0-99565737cea9"
      },
      "source": [
        "# Now, we are ready for predictions:\n",
        "\n",
        "values = [[1.5, 0.1], [1.8, 0.4], [1.3,0.2]]\n",
        "\n",
        "for value in X:\n",
        "    pred = p.predict([value])\n",
        "    print([pred])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([1], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n",
            "[array([0], dtype=int8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daG4gd7MXT6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "43fe586d-37fa-4a85-8b55-4414f10470c3"
      },
      "source": [
        "### Multi-layer Perceptron We will continue with examples using the multilayer perceptron (MLP). The multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers and each layer is fully connected to the following one. The nodes of the layers are neurons using nonlinear activation functions, except for the nodes of the input layer. There can be one or more non-linear hidden layers between the input and the output layer.\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
        "y = [0, 0, 0, 1]\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "print(clf.fit(X, y))   "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCH5u3CPXT_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The following diagram depicts the neural network, that we have trained for our classifier clf. We have two input nodes X0 and X1, called the input layer, and one output neuron 'Out'. We have two hidden layers the first one with the neurons H00 ... H04 and the second hidden layer consisting of H10 and H11. Each neuron of the hidden layers and the output neuron possesses a corresponding Bias, i.e. B00 is the corresponding Bias to the neuron H00, B01 is the corresponding Bias to the neuron H01 and so on.\n",
        "\n",
        "#Each neuron of the hidden layers receives the output from every neuron of the previous layers and transforms these values with a weighted linear summation\n",
        "\n",
        "∑i=0n−1wixi=w0x0+w1x1+...+wn−1xn−1\n",
        "\n",
        "into an output value, where n is the number of neurons of the layer and wi corresponds to the ith component of the weight vector. The output layer receives the values from the last hidden layer. It also performs a linear summation, but a non-linear activation function\n",
        "\n",
        "g(⋅):R→R\n",
        "\n",
        "like the hyperbolic tan function will be applied to the summation result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3MU5KDYYEL9",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.python-course.eu/images/mlp_example_layer.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXDwYhJBXUCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "76473b47-6025-42e4-a83a-817270073c79"
      },
      "source": [
        "# The attribute coefs_ contains a list of weight matrices for every layer. The weight matrix at index i holds the weights between the layer i and layer i + 1.\n",
        "\n",
        "print(\"weights between input and first hidden layer:\")\n",
        "print(clf.coefs_[0])\n",
        "print(\"\\nweights between first hidden and second hidden layer:\")\n",
        "print(clf.coefs_[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights between input and first hidden layer:\n",
            "[[-0.14203691 -1.18304359 -0.85567518 -4.53250719 -0.60466275]\n",
            " [-0.69781111 -3.5850093  -0.26436018 -4.39161248  0.06644423]]\n",
            "\n",
            "weights between first hidden and second hidden layer:\n",
            "[[ 0.29179638 -0.14155284]\n",
            " [ 4.02666592 -0.61556475]\n",
            " [-0.51677234  0.51479708]\n",
            " [ 7.37215202 -0.31936965]\n",
            " [ 0.32920668  0.64428109]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLe7K6ESXT9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The summation formula of the neuron H00 is defined by:\n",
        "\n",
        "# ∑i=0n−1wixi=w0x0+w1x1+wB11∗B11\n",
        "\n",
        "# which can be written as\n",
        "\n",
        "# ∑i=0n−1wixi=w0x0+w1x1+wB11\n",
        "\n",
        "# because B11=1.\n",
        "\n",
        "# We can get the values for w0 and w1 from clf.coefs_ like this:\n",
        "\n",
        "# w0 = clf.coefs_[0][0][0] and w1= clf.coefs_[0][1][0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylp39xRAW-tI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "717f0582-320a-4688-9cc0-0a604cc471b1"
      },
      "source": [
        "print(\"w0 = \", clf.coefs_[0][0][0])\n",
        "print(\"w1 = \", clf.coefs_[0][1][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0 =  -0.14203691267827168\n",
            "w1 =  -0.6978111149778693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNAA-j83W-z-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a30c934a-df03-4539-ee7d-367abfe029a2"
      },
      "source": [
        "# The weight vector of H00 can be accessed with\n",
        "\n",
        "clf.coefs_[0][:,0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.14203691, -0.69781111])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-oQPvrDW-xj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "617fdc49-160b-4a04-94df-9feb597c9db3"
      },
      "source": [
        "# We can generalize the above to access a neuron Hij in the following way:\n",
        "\n",
        "for i in range(len(clf.coefs_)):\n",
        "    number_neurons_in_layer = clf.coefs_[i].shape[1]\n",
        "    for j in range(number_neurons_in_layer):\n",
        "        weights = clf.coefs_[i][:,j]\n",
        "        print(i, j, weights, end=\", \")\n",
        "        print()\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 [-0.14203691 -0.69781111], \n",
            "0 1 [-1.18304359 -3.5850093 ], \n",
            "0 2 [-0.85567518 -0.26436018], \n",
            "0 3 [-4.53250719 -4.39161248], \n",
            "0 4 [-0.60466275  0.06644423], \n",
            "\n",
            "1 0 [ 0.29179638  4.02666592 -0.51677234  7.37215202  0.32920668], \n",
            "1 1 [-0.14155284 -0.61556475  0.51479708 -0.31936965  0.64428109], \n",
            "\n",
            "2 0 [-4.96774269 -0.86330397], \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL31YqH3W-v5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9b319a6f-6542-4090-bb24-bb93d89c1a57"
      },
      "source": [
        "#  intercepts_ is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1.\n",
        "\n",
        "print(\"Bias values for first hidden layer:\")\n",
        "print(clf.intercepts_[0])\n",
        "print(\"\\nBias values for second hidden layer:\")\n",
        "print(clf.intercepts_[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bias values for first hidden layer:\n",
            "[-0.14962269 -0.59232707 -0.5472481   7.02667699 -0.87510813]\n",
            "\n",
            "Bias values for second hidden layer:\n",
            "[-3.61417672 -0.76834882]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A27mVAURW-rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "97fd009d-ebf1-4021-d947-fe72700bb1b0"
      },
      "source": [
        "# The main reason, why we train a classifier is to predict results for new samples. We can do this with the predict method. The method returns a predicted class for a sample, in our case a \"0\" or a \"1\" :\n",
        "\n",
        "result = clf.predict([[0, 0], [0, 1], \n",
        "                      [1, 0], [0, 1], \n",
        "                      [1, 1], [2., 2.],\n",
        "                      [1.3, 1.3], [2, 4.8]])\n",
        "\n",
        "# Instead of just looking at the class results, we can also use the predict_proba method to get the probability estimates.\n",
        "\n",
        "prob_results = clf.predict_proba([[0, 0], [0, 1], \n",
        "                                  [1, 0], [0, 1], \n",
        "                                  [1, 1], [2., 2.], \n",
        "                                  [1.3, 1.3], [2, 4.8]])\n",
        "print(prob_results)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+000 5.25723951e-101]\n",
            " [1.00000000e+000 3.71534882e-031]\n",
            " [1.00000000e+000 6.47069178e-029]\n",
            " [1.00000000e+000 3.71534882e-031]\n",
            " [2.07145538e-004 9.99792854e-001]\n",
            " [2.07145538e-004 9.99792854e-001]\n",
            " [2.07145538e-004 9.99792854e-001]\n",
            " [2.07145538e-004 9.99792854e-001]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JknPnnIwW-p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prob_results[i][0] gives us the probability for the class0, i.e. a \"0\" and results[i][1] the probabilty for a \"1\". i corresponds to the ith sample."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgoV7hFYWgb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9e5e035b-3ea6-400b-c941-7c6a3d26e6e0"
      },
      "source": [
        "# Another Example\n",
        "#We will populate two clusters (class0 and class1) in a two dimensional space.\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "npoints = 50\n",
        "X, Y = [], []\n",
        "# class 0\n",
        "X.append(np.random.uniform(low=-2.5, high=2.3, size=(npoints,)) )\n",
        "Y.append(np.random.uniform(low=-1.7, high=2.8, size=(npoints,)))\n",
        "\n",
        "# class 1\n",
        "X.append(np.random.uniform(low=-7.2, high=-4.4, size=(npoints,)) )\n",
        "Y.append(np.random.uniform(low=3, high=6.5, size=(npoints,)))\n",
        "\n",
        "learnset = []\n",
        "learnlabels = []\n",
        "for i in range(2):\n",
        "    # adding points of class i to learnset\n",
        "    points = zip(X[i], Y[i])\n",
        "    for p in points:\n",
        "        learnset.append(p)\n",
        "        learnlabels.append(i)\n",
        "\n",
        "npoints_test = 3 * npoints\n",
        "TestX = np.random.uniform(low=-7.2, high=5, size=(npoints_test,)) \n",
        "TestY = np.random.uniform(low=-4, high=9, size=(npoints_test,))\n",
        "testset = []\n",
        "points = zip(TestX, TestY)\n",
        "for p in points:\n",
        "    testset.append(p)\n",
        "\n",
        "\n",
        "colours = [\"b\", \"r\"]\n",
        "for i in range(2):\n",
        "    plt.scatter(X[i], Y[i], c=colours[i])\n",
        "plt.scatter(TestX, TestY, c=\"g\")\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzNOK0MuZlsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will train a MLPClassifier for our two classes:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_mldata\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(20, 3), max_iter=150, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "mlp.fit(learnset, learnlabels)\n",
        "print(\"Training set score: %f\" % mlp.score(learnset, learnlabels))\n",
        "print(\"Test set score: %f\" % mlp.score(learnset, learnlabels))\n",
        "\n",
        "\n",
        "mlp.classes_"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxjSBe36ZlvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = clf.predict(testset)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlmP2a7gZl0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "testset = np.array(testset)\n",
        "testset[predictions==1]\n",
        "\n",
        "\n",
        "colours = ['#C0FFFF', \"#FFC8C8\"]\n",
        "for i in range(2):\n",
        "    plt.scatter(X[i], Y[i], c=colours[i])\n",
        "\n",
        "\n",
        "colours = [\"b\", \"r\"]\n",
        "for i in range(2):\n",
        "    cls = testset[predictions==i]\n",
        "    Xt, Yt = zip(*cls)\n",
        "    plt.scatter(Xt, Yt, marker=\"D\", c=colours[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-XcPdMfZlyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MNIST Dataset\n",
        "#We have already used the MNIST dataset in the chapter Testing with MNIST of our tutorial. You will also find some explanations about this dataset.\n",
        "#We want to apply the MLPClassifier on the MNIST data. We can load in the data with pickle:\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"data/mnist/pickled_mnist.pkl\", \"br\") as fh:\n",
        "    data = pickle.load(fh)\n",
        "\n",
        "train_imgs = data[0]\n",
        "test_imgs = data[1]\n",
        "train_labels = data[2]\n",
        "test_labels = data[3]\n",
        "train_labels_one_hot = data[4]\n",
        "test_labels_one_hot = data[5]\n",
        "\n",
        "image_size = 28 # width and length\n",
        "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
        "image_pixels = image_size * image_size\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, ), \n",
        "                    max_iter=480, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, \n",
        "                    tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "train_labels = train_labels.reshape(train_labels.shape[0],)\n",
        "print(train_imgs.shape, train_labels.shape)\n",
        "\n",
        "mlp.fit(train_imgs, train_labels)\n",
        "print(\"Training set score: %f\" % mlp.score(train_imgs, train_labels))\n",
        "print(\"Test set score: %f\" % mlp.score(test_imgs, test_labels))\n",
        "\n",
        "help(mlp.fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrwOLmhaJKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(4, 4)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
        "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
        "               vmax=.5 * vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YGV6cuAWgey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit\n",
        "# Scikit-learn is a Python module merging classic machine learning algorithms with the world of scientific Python packages (NumPy, SciPy, matplotlib)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdnLL51bf67k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf9f11ba-c0d1-4414-e447-2dad106235cc"
      },
      "source": [
        "# Our Learning Set: \"digits\"\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "#iris = datasets.load_iris()\n",
        "digits = datasets.load_digits()\n",
        "print(type(digits))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'sklearn.utils.Bunch'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjAz-hKhgDGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2189e5c0-70d9-4f44-e425-6dbcf1fc8591"
      },
      "source": [
        "print(digits.data)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  0.  5. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ... 10.  0.  0.]\n",
            " [ 0.  0.  0. ... 16.  9.  0.]\n",
            " ...\n",
            " [ 0.  0.  1. ...  6.  0.  0.]\n",
            " [ 0.  0.  2. ... 12.  0.  0.]\n",
            " [ 0.  0. 10. ... 12.  1.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cacNrqtWgDL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66d6dbe3-fdee-4951-e0d0-2a04ae5c2e05"
      },
      "source": [
        "# digits.data contains the features, i.e. images of handwritten images of digits, which can be used for classification.\n",
        "\n",
        "digits.target"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, ..., 8, 9, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChADiZwagDJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0fbba9ea-ba89-43bf-dfb2-f941f18a00d4"
      },
      "source": [
        "# digits.target contain the labels, i.e. digits from 0 to 9 for the digits of digits.data. The data \"digits\" is a 2 D array with the shape (number of samples, number of features). In our case, a sample is an image of shape (8, 8):\n",
        "\n",
        "print(digits.target[0], digits.data[0])\n",
        "print(digits.images[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
            " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
            "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
            "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
            "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
            " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
            " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
            " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
            " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
            " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
            " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
            " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8SqcCOyf7DV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dd258b36-13a0-493f-9f86-6d7d3d3a8675"
      },
      "source": [
        "# Learning and Predicting\n",
        "#We want to predict for a given image, which digit it depicts. Our data set contains samples for the classes 0 (zero) to 9 (nine). We will use these samples to fit an estimator so that we can predict unseen samples as well.\n",
        "#In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X,y) and predict(T).\n",
        "#An example of an estimator is the class sklearn.svm.SVC that implements support vector classification. The constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider the estimator as a black box:\n",
        "\n",
        "from sklearn import svm            # import support vector machine\n",
        "classifier = svm.SVC(gamma=0.001, C=100.)\n",
        "\n",
        "classifier.fit(digits.data[:-3], digits.target[:-3])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DFGFhN3f7In",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99c84b11-d831-48f4-de5b-11039c29b631"
      },
      "source": [
        "#The classifier, which we have created with svm.SVC, is an estimator object. In general the scikit-learn API provides estimator objects, which can be any object that can learn from data. Learning can be done by classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data.\n",
        "#All estimator objects expose a fit method that takes a dataset (usually a 2-d array):\n",
        "\n",
        "classifier.predict(digits.data[-3:])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 9, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW5gIlt5f7Ga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8161871a-51f5-4fa6-eda0-2cc92a16e2ec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "img = Image.fromarray(np.uint8(digits.images[-2]))\n",
        "\n",
        "plt.gray()\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KVCVsGPf7BC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cfda3dee-190f-487f-9ae8-ba3933c139c6"
      },
      "source": [
        "#Iris Dataset\n",
        "#The Iris flower data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\"\n",
        "#The data set consists of 50 samples from each of three species of Iris\n",
        "\n",
        "#Iris setosa,\n",
        "#Iris virginica and\n",
        "#Iris versicolor).\n",
        "#Four features were measured from each sample the length and the width of the sepals and petals, in centimetres.\n",
        "#Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
        "\n",
        "# Saving Trained Models\n",
        "#It's possible to keep a trained model persistently with the pickle module.\n",
        "#In the following example, we want to demonstrate how to learn a classifier and save it for later usage with the pickle module of Python:\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "import pickle\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKbZ69o3hNip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = open(\"classifiers/iris.pkl\", \"bw\")\n",
        "pickle.dump(clf, fname)\n",
        "\n",
        "# load the saved classifier:\n",
        "fname = open(\"classifiers/iris.pkl\", \"br\")\n",
        "clf2 = pickle.load(fname)\n",
        "clf2.predict(iris.data[::5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-2SwxdBhNqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, we will do the same with joblib package from sklearn.externals. joblib is more efficient on big data:\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "joblib.dump(clf, 'classifiers/iris2.pkl')\n",
        "\n",
        "clf3 = joblib.load('classifiers/iris2.pkl')\n",
        "\n",
        "clf3.predict(iris.data[::5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaIX24OshNu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Statistical-learning for Scientific Data Processing\n",
        "#We saw that the \"iris dataset\" consists of 150 observations of irises, i.e. the samples. Each oberservation is described by four features (the length and the width of the sepals and petals).\n",
        "#In general, we can say that Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. Such an array can be seen as a list of multi-dimensional observations. The first axis of such an array is the samples axis and the second one is the features axis.\n",
        "\n",
        "#Supervised Learning\n",
        "#Supervised learning consists in the task of finding or deducing a function from labeled training data. The training data consist of a set of training examples. In other words: We have the actual data X and the corresponding \"targets\" y, also called \"labels\". Often y is a one dimensional array.\n",
        "#An estimator in scikit-learn provides a fit method to fit the model: fit(X, y). It also supplies a predict method which returns predicted labels y for (unlabeled) observations X: predict(X) --> y.\n",
        "\n",
        "#Instance Based Learning -- -k-nearest-neighbor\n",
        "#Instance based learning works directly on the learned samples, instead of creating rules compared to other classification methods.\n",
        "#Way of working: Each new instance is compared with the already existing instances. The instances are compared by using a distance metric. The instance with the closest distance value detwermines the class for the new instance. This classification method is called nearest-neighbor classification.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "iris_X = iris.data\n",
        "iris_y = iris.target\n",
        "print(iris_X[:8])\n",
        "\n",
        "#We create a learnsetfrom the sets above. We use permutation from np.random to split the data randomly:\n",
        "\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(iris_X))\n",
        "n_training_samples = 12\n",
        "iris_X_train = iris_X[indices[:-n_training_samples]]\n",
        "iris_y_train = iris_y[indices[:-n_training_samples]]\n",
        "iris_X_test = iris_X[indices[-n_training_samples:]]\n",
        "iris_y_test = iris_y[indices[-n_training_samples:]]\n",
        "print(iris_X_test)\n",
        "\n",
        "\n",
        "#To determine the similarity between to instances, we need a distance function. In our example, the Euclidean distance is ideal:\n",
        "\n",
        "def distance(instance1, instance2):\n",
        "    # just in case, if the instances are lists or tuples:\n",
        "    instance1 = np.array(instance1) \n",
        "    instance2 = np.array(instance2)\n",
        "    \n",
        "    return np.linalg.norm(instance1 - instance2)\n",
        "\n",
        "print(distance([4, 3, 2], [1, 1,1]))\n",
        "\n",
        "def get_neighbors(training_set, test_instance, k):\n",
        "    distances = []\n",
        "    for training_instance in training_set:\n",
        "        dist = distance(test_instance, training_instance[:-1])\n",
        "        distances.append((training_instance, dist))\n",
        "    distances.sort(key=lambda x: x[1])\n",
        "    neighbors = []\n",
        "    for i in range(k):\n",
        "        neighbors.append(distances[i][0])\n",
        "    return neighbors\n",
        "\n",
        "train_set = [(1, 2, 2, 'apple'), \n",
        "             (-3, -2, 0,  'banana'),\n",
        "             (1, 1, 3, 'apple'), \n",
        "             (-3, -3, -1,  'banana')\n",
        "            ]\n",
        "\n",
        "k = 1\n",
        "for test_instance in [(0, 0, 0), (2, 2, 2), (-3, -1, 0)]:\n",
        "    neighbors = get_neighbors(train_set, test_instance, 2)\n",
        "    print(test_instance, neighbors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKhIebAEi2G7",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"nb\"></a>\n",
        "\n",
        "### Naive Bayes Classifier\n",
        "\n",
        "\n",
        "\n",
        "Naive Bayes Classifier\n",
        "\n",
        "In machine learning, a Bayes classifier is a simple probabilistic classifier, which is based on applying Bayes' theorem. The feature model used by a naive Bayes classifier makes strong independence assumptions. This means that the existence of a particular feature of a class is independent or unrelated to the existence of every other feature.\n",
        "\n",
        "Definition of independent events:\n",
        "\n",
        "Two events E and F are independent, if both E and F have positive probability and if P(E|F) = P(E) and P(F|E) = P(F)\n",
        "\n",
        "As we have stated in our definition, the Naive Bayes Classifier is based on the Bayes' theorem. The Bayes theorem is based on the conditional probability, which we will define now:\n",
        "\n",
        "Conditional Probability\n",
        "\n",
        "P(A|B) stands for \"the conditional probability of A given B\", or \"the probability of A under the condition B\", i.e. the probability of some event A under the assumption that the event B took place. When in a random experiment the event B is known to have occurred, the possible outcomes of the experiment are reduced to B, and hence the probability of the occurrence of A is changed from the unconditional probability into the conditional probability given B. The Joint probability is the probability of two events in conjunction. That is, it is the probability of both events together. There are three notations for the joint probability of A and B. It can be written as\n",
        "\n",
        "P(A∩B)\n",
        "P(AB) or\n",
        "P(A,B)\n",
        "The conditional probability is defined by\n",
        "\n",
        "P(A|B)=P(A∩B)P(B)\n",
        "Examples\n",
        "\n",
        "\n",
        "German Swiss Speaker\n",
        "There are about 8.4 million people living in Switzerland. About 64 % of them speak German. There are about 7500 million people on earth.\n",
        "\n",
        "If some aliens randomly beam up an earthling, what are the chances that he is a German speaking Swiss?\n",
        "\n",
        "We have the events\n",
        "\n",
        "S: being Swiss\n",
        "\n",
        "GS: German Speaking\n",
        "\n",
        "The probability for a randomly chosen person to be Swiss:\n",
        "\n",
        "P(S)=8.47500=0.00112\n",
        "If we know that somebody is Swiss, the probability of speaking German is 0.64. This corresponds to the conditional probability\n",
        "\n",
        "P(GS|S)=0.64\n",
        "So the probability of the earthling being Swiss and speaking German, can be calculated by the formula:\n",
        "\n",
        "P(GS|S)=P(GS∩S)P(S)\n",
        "inserting the values from above gives us:\n",
        "\n",
        "0.64=P(GS∩S)0.00112\n",
        "and\n",
        "\n",
        "P(GS∩S)=0.0007168\n",
        "So our aliens end up with a chance of 0.07168 % of getting a German speaking Swiss person.\n",
        "\n",
        "False Positives and False Negatives\n",
        "A medical research lab proposes a screening to test a large group of people for a disease. An argument against such screenings is the problem of false positive screening results.\n",
        "\n",
        "Suppose 0,1% of the group suffer from the disease, and the rest is well:\n",
        "\n",
        "P(\"sick\")=0,1\n",
        "and\n",
        "P(\"well\")=99,9\n",
        "The following is true for a screening test:\n",
        "\n",
        "If you have the disease, the test will be positive 99% of the time, and if you don't have it, the test will be negative 99% of the time:\n",
        "\n",
        "P(\"test positive\" | \"well\") = 1 %\n",
        "\n",
        "and\n",
        "\n",
        "P(\"test negative\" | \"well\") = 99 %.\n",
        "\n",
        "Finally, suppose that when the test is applied to a person having the disease, there is a 1% chance of a false negative result (and 99% chance of getting a true positive result), i.e.\n",
        "\n",
        "P(\"test negative\" | \"sick\") = 1 %\n",
        "\n",
        "and\n",
        "\n",
        "P(\"test positive\" | \"sick\") = 99 %\n",
        "\n",
        "\n",
        "Sick\n",
        "Healthy\n",
        "Totals\n",
        "Test result positive\n",
        "99\n",
        "999\n",
        "1098\n",
        "Test result negative\n",
        "1\n",
        "98901\n",
        "98902\n",
        "Totals\n",
        "100\n",
        "99900\n",
        "100000\n",
        "There are 999 False Positives and 1 False Negative.\n",
        "\n",
        "Problem:\n",
        "\n",
        "In many cases even medical professionals assume that \"if you have this sickness, the test will be positive in 99 % of the time and if you don't have it, the test will be negative 99 % of the time. Out of the 1098 cases that report positive results only 99 (9 %) cases are correct and 999 cases are false positives (91 %), i.e. if a person gets a positive test result, the probability that he or she actually has the disease is just about 9 %. P(\"sick\" | \"test positive\") = 99 / 1098 = 9.02 %\n",
        "\n",
        "\n",
        "Let's set out on a journey by train to create our first very simple Naive Bayes Classifier. Let us assume we are in the city of Hamburg and we want to travel to Munich. We will have to change trains in Frankfurt am Main. We know from previous train journeys that our train from Hamburg might be delayed and the we will not catch our connecting train in Frankfurt. The probability that we will not be in time for our connecting train depends on how high our possible delay will be. The connecting train will not wait for more than five minutes. Sometimes the other train is delayed as well.\n",
        "\n",
        "The following lists 'in_time' (the train from Hamburg arrived in time to catch the connecting train to Munich) and 'too_late' (connecting train is missed) are data showing the situation over some weeks. The first component of each tuple shows the minutes the train was late and the second component shows the number of time this occurred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzthRyaIhNoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tuples are (minutes, number of times)\n",
        "in_time = [(0, 22), (1, 19), (2, 17), (3, 18),\n",
        "           (4, 16), (5, 15), (6, 9), (7, 7),\n",
        "           (8, 4), (9, 3), (10, 3), (11, 2)]\n",
        "too_late = [(6, 6), (7, 9), (8, 12), (9, 17), \n",
        "            (10, 18), (11, 15), (12,16), (13, 7),\n",
        "            (14, 8), (15, 5)]\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, Y = zip(*in_time)\n",
        "\n",
        "X2, Y2 = zip(*too_late)\n",
        "\n",
        "bar_width = 0.9\n",
        "plt.bar(X, Y, bar_width,  color=\"blue\", alpha=0.75, label=\"in time\")\n",
        "bar_width = 0.8\n",
        "plt.bar(X2, Y2, bar_width,  color=\"red\", alpha=0.75, label=\"too late\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yzWr9cGi1r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From this data we can deduce that the probability of catching the connecting train if we are one minute late is 1, because we had 19 successful cases experienced and no misses, i.e. there is no tuple with 1 as the first component in 'too_late'.\n",
        "#We will denote the event \"train arrived in time to catch the connecting train\" with S (success) and the 'unlucky' event \"train arrived too late to catch the connecting train\" with M (miss)\n",
        "#We can now define the probability \"catching the train given that we are 1 minute late\" formally:\n",
        "\n",
        "P(S|1)=19/19=1\n",
        "#We used the fact that the tuple (1,19) is in 'in_time' and there is no tuple with the first component 1 in 'too_late'\n",
        "#It's getting critical for catching the connecting train to Munich, if we are 6 minutes late. Yet, the chances are still 60 %:\n",
        "\n",
        "P(S|6)=9/9+6=0.6\n",
        "\n",
        "#Accordingly, the probability for missing the train knowing that we are 6 minutes late is:\n",
        "\n",
        "P(M|6)=6/9+6=0.4\n",
        "\n",
        "#We can write a 'classifier' function, which will give the probability for catching the connecting train:\n",
        "\n",
        "in_time_dict = dict(in_time)\n",
        "too_late_dict = dict(too_late)\n",
        "\n",
        "def catch_the_train(min):\n",
        "    s = in_time_dict.get(min, 0)\n",
        "    if s == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        m = too_late_dict.get(min, 0)\n",
        "        return s / (s + m)\n",
        "\n",
        "for minutes in range(-1, 13):\n",
        "    print(minutes, catch_the_train(minutes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22tCeSVli11w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A Naive Bayes Classifier Example\n",
        "#Getting the Data Ready\n",
        "#We will use a file called 'person_data.txt'. It contains 100 random person data, male and female, with body sizes, weights and gender tags.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "genders = [\"male\", \"female\"]\n",
        "persons = []\n",
        "with open(\"data/person_data.txt\") as fh:\n",
        "    for line in fh:\n",
        "        persons.append(line.strip().split())\n",
        "\n",
        "firstnames = {}\n",
        "heights = {}\n",
        "for gender in genders:\n",
        "    firstnames[gender] = [ x[0] for x in persons if x[4]==gender]\n",
        "    heights[gender] = [ x[2] for x in persons if x[4]==gender]\n",
        "    heights[gender] = np.array(heights[gender], np.int)\n",
        "    \n",
        "for gender in (\"female\", \"male\"):\n",
        "    print(gender + \":\")\n",
        "    print(firstnames[gender][:10])\n",
        "    print(heights[gender][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaV_gWe1i1zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Designing a Feature class\n",
        "#We will now define a Python class \"Feature\" for the features, which we will use for classification later.\n",
        "#The Feature class needs a label, e.g. \"heights\" or \"firstnames\". If the feature values are numerical we may want to \"bin\" them to reduce the number of possible feature values. The heights from our persons have a huge range and we have only 50 measured values for our Naive Bayes classes \"male\" and \"female\". We will bin them into ranges \"130 to 134\", \"135 to 139\", \"140 to 144\" and so on by setting bin_width to 5. There is no way of binning the first names, so bin_width will be set to None.\n",
        "#The method frequency returns the number of occurrencies for a certain feature value or a binned range.\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class Feature:\n",
        "    \n",
        "    def __init__(self, data, name=None, bin_width=None):\n",
        "        self.name = name\n",
        "        self.bin_width = bin_width\n",
        "        if bin_width:\n",
        "            self.min, self.max = min(data), max(data)\n",
        "            bins = np.arange((self.min // bin_width) * bin_width, \n",
        "                                (self.max // bin_width) * bin_width,\n",
        "                                bin_width)\n",
        "            freq, bins = np.histogram(data, bins)\n",
        "            self.freq_dict = dict(zip(bins, freq))\n",
        "            self.freq_sum = sum(freq)\n",
        "        else:\n",
        "            self.freq_dict = dict(Counter(data))\n",
        "            self.freq_sum = sum(self.freq_dict.values())\n",
        "            \n",
        "        \n",
        "        \n",
        "    def frequency(self, value):\n",
        "        if self.bin_width:\n",
        "            value = (value // self.bin_width) * self.bin_width\n",
        "        if value in self.freq_dict:\n",
        "            return self.freq_dict[value]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "#We will create now two feature classes Feature for the height values of the person data set. One Feature class contains the height for the Naive Bayes class \"male\" and one the heights for the class \"female\":\n",
        "\n",
        "fts = {}\n",
        "for gender in genders:\n",
        "    fts[gender] = Feature(heights[gender], name=gender, bin_width=5)\n",
        "    print(gender, fts[gender].freq_dict)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_MYhqypi1xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will create now two feature classes Feature for the height values of the person data set. One Feature class contains the height for the Naive Bayes class \"male\" and one the heights for the class \"female\":\n",
        "\n",
        "fts = {}\n",
        "for gender in genders:\n",
        "    fts[gender] = Feature(heights[gender], name=gender, bin_width=5)\n",
        "    print(gender, fts[gender].freq_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkaSMHBCi1uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We have to design now a Naive Bayes class in Python. We will call it NBclass. An NBclass contains one or more Feature classes. The name of the NBclass will be stored in self.name.\n",
        "\n",
        "class NBclass:\n",
        "        \n",
        "        def __init__(self, name, *features):\n",
        "            self.features = features\n",
        "            self.name = name\n",
        "            \n",
        "        def probability_value_given_feature(self, \n",
        "                                            feature_value,\n",
        "                                            feature):\n",
        "            \"\"\"\n",
        "            p_value_given_feature returns the probability p \n",
        "            for a feature_value 'value' of the feature  to occurr\n",
        "            corresponds to P(d_i | p_j)\n",
        "            where d_i is a feature variable of the feature i\n",
        "            \"\"\"\n",
        "            \n",
        "            if feature.freq_sum == 0:\n",
        "                return 0\n",
        "            else:\n",
        "                return feature.frequency(feature_value) / feature.freq_sum\n",
        "\n",
        "#In the following code, we will create NBclasses with one feature, i.e. the height feature. We will use the Feature classes of fts, which we have previously created:\n",
        "\n",
        "cls = {}\n",
        "for gender in genders:\n",
        "    cls[gender] = NBclass(gender, fts[gender])                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZCivDkohNmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The final step for creating a simple Naive Bayes classifier consists in writing a class 'Classifier', which will use our classes 'NBclass' and 'Feature'.\n",
        "\n",
        "class Classifier:\n",
        "    \n",
        "    def __init__(self, *nbclasses):\n",
        "        self.nbclasses = nbclasses\n",
        "        \n",
        "        \n",
        "    def prob(self, *d, best_only=True):\n",
        "        \n",
        "        nbclasses = self.nbclasses\n",
        "        probability_list = []\n",
        "        for nbclass in nbclasses:            \n",
        "            ftrs = nbclass.features\n",
        "            prob = 1\n",
        "            for i in range(len(ftrs)):\n",
        "                prob *= nbclass.probability_value_given_feature(d[i], ftrs[i])\n",
        "              \n",
        "            probability_list.append( (prob, nbclass.name) )\n",
        "\n",
        "        prob_values = [f[0] for f in probability_list]\n",
        "        prob_sum = sum(prob_values)\n",
        "        if prob_sum==0:\n",
        "            number_classes = len(self.nbclasses)\n",
        "            pl = []\n",
        "            for prob_element in probability_list:\n",
        "                pl.append( ((1 / number_classes), prob_element[1]))\n",
        "            probability_list = pl\n",
        "        else:\n",
        "            probability_list = [ (p[0] / prob_sum, p[1])  for p in probability_list]\n",
        "        if best_only:\n",
        "            return max(probability_list)\n",
        "        else:\n",
        "            return probability_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWHXlKKVf6_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will create a classifier with one feature class 'height'. We check it with values between 130 and 220 cm.\n",
        "\n",
        "c = Classifier(cls[\"male\"], cls[\"female\"])\n",
        "\n",
        "for i in range(130, 220, 5):\n",
        "    print(i, c.prob(i, best_only=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CdnrI6bWgac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#There are no persons - neither male nor female - in our learn set, with a body height between 140 and 144. This is the reason, why our classifier can't base its result on learned data and therefore comes back with a fify-fifty result.\n",
        "#We can also train a classifier with our firstnames:\n",
        "\n",
        "fts = {}\n",
        "cls = {}\n",
        "for gender in genders:\n",
        "    fts_names = Feature(firstnames[gender], name=gender)\n",
        "    cls[gender] = NBclass(gender, fts_names)\n",
        "    \n",
        "c = Classifier(cls[\"male\"], cls[\"female\"])\n",
        "\n",
        "testnames = ['Edgar', 'Benjamin', 'Fred', 'Albert', 'Laura', \n",
        "             'Maria', 'Paula', 'Sharon', 'Jessie']\n",
        "for name in testnames:\n",
        "    print(name, c.prob(name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIJfjNqlGu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The name \"Jessie\" is an ambiguous name. There are about 66 boys per 100 girls with this name. We can learn from the previous classification results that the probability for the name \"Jessie\" being \"female\" is about two-thirds, which is calculated from our data set \"person\":\n",
        "\n",
        "[person for person in persons if person[0] == \"Jessie\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGyyWkXhlGsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Jessie Washington is only 159 cm tall. If we have a look at the results of our Classifier, trained with heights, we see that the likelihood for a person 159 cm tall of being \"female\" is 0.875. So what about an unknown person called \"Jessie\" and being 159 cm tall? Is this person female or male?\n",
        "#To answer this question, we will train an Naive Bayes classifier with two feature classes, i.e. heights and firstnames:\n",
        "\n",
        "cls = {}\n",
        "for gender in genders:\n",
        "    fts_heights = Feature(heights[gender], name=\"heights\", bin_width=5)\n",
        "    fts_names = Feature(firstnames[gender], name=\"names\")\n",
        "\n",
        "    cls[gender] = NBclass(gender, fts_names, fts_heights)\n",
        "\n",
        "\n",
        "c = Classifier(cls[\"male\"], cls[\"female\"])\n",
        "\n",
        "for d in [(\"Maria\", 140), (\"Anthony\", 200), (\"Anthony\", 153), \n",
        "          (\"Jessie\", 188) , (\"Jessie\", 159), (\"Jessie\", 160) ]:\n",
        "    print(d, c.prob(*d, best_only=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Lf25naleCT",
        "colab_type": "text"
      },
      "source": [
        "The Underlying Theory\n",
        "Our classifier from the previous example is based on the Bayes theorem:\n",
        "\n",
        "P(cj|d)=P(d|cj)P(cj)P(d)\n",
        "where\n",
        "\n",
        "P(cj|d) is the probability of instance d being in class c_j, it is the result we want to calculate with our classifier\n",
        "\n",
        "P(d|cj) is the probability of generating the instance d, if the class cj is given\n",
        "\n",
        "P(cj) is the probability for the occurrence of class cj We didn't use it in our classifiers, because both classes in our example have been equally likely.\n",
        "\n",
        "P(d) is the probability for the occurrence of an instance d It's not needed in the calculation, because it is the same for all classes.\n",
        "We had used only one feature in our previous examples, i.e. the 'height' or the name.\n",
        "\n",
        "It's possible to define a Bayes Classifier with multiple features, e.g. d=(d1,d2,...,dn)\n",
        "We get the following formula:\n",
        "\n",
        "P(cj|d)=1P(d)∏i=1nP(di|cj)P(cj)\n",
        "1P(d) is only depending on the values of d1,d2,...dn. This means that it is a constant as the values of the feature variables are known."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK9ep2sJlGrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## sklearn\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# load the iris datasets\n",
        "dataset = datasets.load_iris()\n",
        "# fit a Naive Bayes model to the data\n",
        "model = GaussianNB()\n",
        "\n",
        "model.fit(dataset.data, dataset.target)\n",
        "print(model)\n",
        "# make predictions\n",
        "expected = dataset.target\n",
        "predicted = model.predict(dataset.data)\n",
        "# summarize the fit of the model\n",
        "print(metrics.classification_report(expected, predicted))\n",
        "print(metrics.confusion_matrix(expected, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaI44rLwlfH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use our person data from the previous chapter of our tutorial to train another classifier in the next example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def prepare_person_dataset(fname):\n",
        "    genders = [\"male\", \"female\"]\n",
        "    persons = []\n",
        "    with open(fname) as fh:\n",
        "        for line in fh:\n",
        "            persons.append(line.strip().split())\n",
        "\n",
        "    firstnames = []\n",
        "    dataset = []  # weight and height\n",
        "\n",
        "\n",
        "    for person in persons:\n",
        "        firstnames.append( (person[0], person[4]) )\n",
        "        height_weight = (float(person[2]), float(person[3]))\n",
        "        dataset.append( (height_weight, person[4]))\n",
        "    return dataset\n",
        "\n",
        "learnset = prepare_person_dataset(\"data/person_data.txt\")\n",
        "testset = prepare_person_dataset(\"data/person_data_testset.txt\")\n",
        "print(learnset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o1APjBMlfUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gaussian Naive Bayes\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model = GaussianNB()\n",
        "#print(dataset.data, dataset.target)\n",
        "w, l = zip(*learnset)\n",
        "w = np.array(w)\n",
        "l = np.array(l)\n",
        "\n",
        "\n",
        "model.fit(w, l)\n",
        "print(model)\n",
        "\n",
        "w, l = zip(*testset)\n",
        "w = np.array(w)\n",
        "l = np.array(l)\n",
        "predicted = model.predict(w)\n",
        "print(predicted)\n",
        "print(l)\n",
        "# summarize the fit of the model\n",
        "print(metrics.classification_report(l, predicted))\n",
        "print(metrics.confusion_matrix(l, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0BiWInHlfXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Text classification\n",
        "\n",
        "class BagOfWords(object):\n",
        "    \"\"\" Implementing a bag of words, words corresponding with their frequency of usages in a \"document\"\n",
        "    for usage by the Document class, DocumentClass class and the Pool class.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.__number_of_words = 0\n",
        "        self.__bag_of_words = {}\n",
        "        \n",
        "    def __add__(self,other):\n",
        "        \"\"\" Overloading of the \"+\" operator to join two BagOfWords \"\"\"\n",
        "        erg = BagOfWords()\n",
        "        sum = erg.__bag_of_words\n",
        "        for key in self.__bag_of_words:\n",
        "            sum[key] = self.__bag_of_words[key]\n",
        "            if key in other.__bag_of_words:\n",
        "                sum[key] += other.__bag_of_words[key]\n",
        "        for key in other.__bag_of_words:\n",
        "            if key not in sum:\n",
        "                sum[key] = other.__bag_of_words[key]\n",
        "        return erg\n",
        "        \n",
        "    def add_word(self,word):\n",
        "        \"\"\" A word is added in the dictionary __bag_of_words\"\"\"\n",
        "        self.__number_of_words += 1\n",
        "        if word in self.__bag_of_words:\n",
        "            self.__bag_of_words[word] += 1\n",
        "        else:\n",
        "            self.__bag_of_words[word] = 1\n",
        "    \n",
        "    def len(self):\n",
        "        \"\"\" Returning the number of different words of an object \"\"\"\n",
        "        return len(self.__bag_of_words)\n",
        "    \n",
        "    def Words(self):\n",
        "        \"\"\" Returning a list of the words contained in the object \"\"\"\n",
        "        return self.__bag_of_words.keys()\n",
        "    \n",
        "        \n",
        "    def BagOfWords(self):\n",
        "        \"\"\" Returning the dictionary, containing the words (keys) with their frequency (values)\"\"\"\n",
        "        return self.__bag_of_words\n",
        "        \n",
        "    def WordFreq(self,word):\n",
        "        \"\"\" Returning the frequency of a word \"\"\"\n",
        "        if word in self.__bag_of_words:\n",
        "            return self.__bag_of_words[word]\n",
        "        else:\n",
        "            return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UshfvD-RnOU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Document(object):\n",
        "    \"\"\" Used both for learning (training) documents and for testing documents. The optional parameter lear\n",
        "    has to be set to True, if a classificator should be trained. If it is a test document learn has to be set to False. \"\"\"\n",
        "    _vocabulary = BagOfWords()\n",
        " \n",
        "    def __init__(self, vocabulary):\n",
        "        self.__name = \"\"\n",
        "        self.__document_class = None\n",
        "        self._words_and_freq = BagOfWords()\n",
        "        Document._vocabulary = vocabulary\n",
        "    \n",
        "    def read_document(self,filename, learn=False):\n",
        "        \"\"\" A document is read. It is assumed that the document is either encoded in utf-8 or in iso-8859... (latin-1).\n",
        "        The words of the document are stored in a Bag of Words, i.e. self._words_and_freq = BagOfWords() \"\"\"\n",
        "        try:\n",
        "            text = open(filename,\"r\", encoding='utf-8').read()\n",
        "        except UnicodeDecodeError:\n",
        "            text = open(filename,\"r\", encoding='latin-1').read()\n",
        "        text = text.lower()\n",
        "        words = re.split(r\"\\W\",text)\n",
        "\n",
        "        self._number_of_words = 0\n",
        "        for word in words:\n",
        "            self._words_and_freq.add_word(word)\n",
        "            if learn:\n",
        "                Document._vocabulary.add_word(word)\n",
        "\n",
        "\n",
        "    def __add__(self,other):\n",
        "        \"\"\" Overloading the \"+\" operator. Adding two documents consists in adding the BagOfWords of the Documents \"\"\"\n",
        "        res = Document(Document._vocabulary)\n",
        "        res._words_and_freq = self._words_and_freq + other._words_and_freq    \n",
        "        return res\n",
        "    \n",
        "    def vocabulary_length(self):\n",
        "        \"\"\" Returning the length of the vocabulary \"\"\"\n",
        "        return len(Document._vocabulary)\n",
        "                \n",
        "    def WordsAndFreq(self):\n",
        "        \"\"\" Returning the dictionary, containing the words (keys) with their frequency (values) as contained\n",
        "        in the BagOfWords attribute of the document\"\"\"\n",
        "        return self._words_and_freq.BagOfWords()\n",
        "        \n",
        "    def Words(self):\n",
        "        \"\"\" Returning the words of the Document object \"\"\"\n",
        "        d =  self._words_and_freq.BagOfWords()\n",
        "        return d.keys()\n",
        "    \n",
        "    def WordFreq(self,word):\n",
        "        \"\"\" Returning the number of times the word \"word\" appeared in the document \"\"\"\n",
        "        bow =  self._words_and_freq.BagOfWords()\n",
        "        if word in bow:\n",
        "            return bow[word]\n",
        "        else:\n",
        "            return 0\n",
        "                \n",
        "    def __and__(self, other):\n",
        "        \"\"\" Intersection of two documents. A list of words occuring in both documents is returned \"\"\"\n",
        "        intersection = []\n",
        "        words1 = self.Words()\n",
        "        for word in other.Words():\n",
        "            if word in words1:\n",
        "                intersection += [word]\n",
        "        return intersection\n",
        "\n",
        "\n",
        "The DocumentClass Class\n",
        "The class DocumentClass is the class for our Document categories. It's inheriting from our Document class.\n",
        "class DocumentClass(Document):\n",
        "    def __init__(self, vocabulary):\n",
        "        Document.__init__(self, vocabulary)\n",
        "        self._number_of_docs = 0\n",
        "\n",
        "    def Probability(self,word):\n",
        "        \"\"\" returns the probabilty of the word \"word\" given the class \"self\" \"\"\"\n",
        "        voc_len = Document._vocabulary.len()\n",
        "        SumN = 0\n",
        "        for i in range(voc_len):\n",
        "            SumN = DocumentClass._vocabulary.WordFreq(word)\n",
        "        N = self._words_and_freq.WordFreq(word)\n",
        "        erg = 1 + N\n",
        "        erg /= voc_len + SumN\n",
        "        return erg\n",
        "\n",
        "    def __add__(self,other):\n",
        "        \"\"\" Overloading the \"+\" operator. Adding two DocumentClass objects consists in adding the \n",
        "        BagOfWords of the DocumentClass objectss \"\"\"\n",
        "        res = DocumentClass(self._vocabulary)\n",
        "        res._words_and_freq = self._words_and_freq + other._words_and_freq \n",
        " \n",
        "        return res\n",
        "\n",
        "    def SetNumberOfDocs(self, number):\n",
        "        self._number_of_docs = number\n",
        "    \n",
        "    def NumberOfDocuments(self):\n",
        "        return self._number_of_docs\n",
        "\n",
        "\n",
        "The Pool class\n",
        "The pool is the class, where the document classes are learnt and kept:\n",
        "class Pool(object):\n",
        "    def __init__(self):\n",
        "        self.__document_classes = {}\n",
        "        self.__vocabulary = BagOfWords()\n",
        "            \n",
        "    def sum_words_in_class(self, dclass):\n",
        "        \"\"\" The number of times all different words of a dclass appear in a class \"\"\"\n",
        "        sum = 0\n",
        "        for word in self.__vocabulary.Words():\n",
        "            WaF = self.__document_classes[dclass].WordsAndFreq()\n",
        "            if word in WaF:\n",
        "                sum +=  WaF[word]\n",
        "        return sum\n",
        "    \n",
        "    def learn(self, directory, dclass_name):\n",
        "        \"\"\" directory is a path, where the files of the class with the name dclass_name can be found \"\"\"\n",
        "        x = DocumentClass(self.__vocabulary)\n",
        "        dir = os.listdir(directory)\n",
        "        for file in dir:\n",
        "            d = Document(self.__vocabulary)\n",
        "            print(directory + \"/\" + file)\n",
        "            d.read_document(directory + \"/\" +  file, learn = True)\n",
        "            x = x + d\n",
        "        self.__document_classes[dclass_name] = x\n",
        "        x.SetNumberOfDocs(len(dir))\n",
        "\n",
        "    \n",
        "    def Probability(self, doc, dclass = \"\"):\n",
        "        \"\"\"Calculates the probability for a class dclass given a document doc\"\"\"\n",
        "        if dclass:\n",
        "            sum_dclass = self.sum_words_in_class(dclass)\n",
        "            prob = 0\n",
        "        \n",
        "            d = Document(self.__vocabulary)\n",
        "            d.read_document(doc)\n",
        "\n",
        "            for j in self.__document_classes:\n",
        "                sum_j = self.sum_words_in_class(j)\n",
        "                prod = 1\n",
        "                for i in d.Words():\n",
        "                    wf_dclass = 1 + self.__document_classes[dclass].WordFreq(i)\n",
        "                    wf = 1 + self.__document_classes[j].WordFreq(i)\n",
        "                    r = wf * sum_dclass / (wf_dclass * sum_j)\n",
        "                    prod *= r\n",
        "                prob += prod * self.__document_classes[j].NumberOfDocuments() / self.__document_classes[dclass].NumberOfDocuments()\n",
        "            if prob != 0:\n",
        "                return 1 / prob\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            prob_list = []\n",
        "            for dclass in self.__document_classes:\n",
        "                prob = self.Probability(doc, dclass)\n",
        "                prob_list.append([dclass,prob])\n",
        "            prob_list.sort(key = lambda x: x[1], reverse = True)\n",
        "            return prob_list\n",
        "\n",
        "    def DocumentIntersectionWithClasses(self, doc_name):\n",
        "        res = [doc_name]\n",
        "        for dc in self.__document_classes:\n",
        "            d = Document(self.__vocabulary)\n",
        "            d.read_document(doc_name, learn=False)\n",
        "            o = self.__document_classes[dc] &  d\n",
        "            intersection_ratio = len(o) / len(d.Words())\n",
        "            res += (dc, intersection_ratio)\n",
        "        return res\n",
        "\n",
        "\n",
        "Using the Classifier\n",
        "To be able to learn and test a classifier, we offer a \"Learn and test set to Download\". The module NaiveBayes consists of the code we have provided so far, but it can be downloaded for convenience as NaiveBayes.py The learn and test sets contain (old) jokes labelled in six categories: \"clinton\", \"lawyer\", \"math\", \"medical\", \"music\", \"sex\".\n",
        "from NaiveBayes import  Pool\n",
        "import os\n",
        "\n",
        "DClasses = [\"clinton\",  \"lawyer\",  \"math\",  \"medical\",  \"music\",  \"sex\"]\n",
        "\n",
        "base = \"learn/\"\n",
        "p = Pool()\n",
        "for i in DClasses:\n",
        "    p.learn(base + i, i)\n",
        "\n",
        "\n",
        "\n",
        "base = \"test/\"\n",
        "for i in DClasses:\n",
        "    dir = os.listdir(base + i)\n",
        "    for file in dir:\n",
        "        res = p.Probability(base + i + \"/\" + file)\n",
        "        print(i + \": \" + file + \": \" + str(res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2N9Ha5YoNs_",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"dt\"></a>\n",
        "\n",
        "### Decision Trees\n",
        "\n",
        "Decision trees are supervised learning algorithms used for both, classification and regression tasks where we will concentrate on classification in this first part of our decision tree tutorial.\n",
        "Decision trees are assigned to the information based learning algorithms which use different measures of information gain for learning. We can use decision trees for issues where we have continuous but also categorical input and target features. The main idea of decision trees is to find those descriptive features which contain the most \"information\" regarding the target feature and then split the dataset along the values of these features such that the target feature values for the resulting sub_datasets are as pure as possible --> The descriptive feature which leaves the target feature most purely is said to be the most informative one. This process of finding the \"most informative\" feature is done until we accomplish a stopping criteria where we then finally end up in so called leaf nodes. The leaf nodes contain the predictions we will make for new query instances presented to our trained model. This is possible since the model has kind of learned the underlying structure of the training data and hence can, given some assumptions, make predictions about the target feature value (class) of unseen query instances.\n",
        "A decision tree mainly contains of a root node, interior nodes, and leaf nodes which are then connected by branches.\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Decision_Tree1.png)\n",
        "\n",
        "Decision trees are further subdivided whether the target feature is continuously scaled like for instance house prices or categorically scaled like for instance animal species.\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Categorical_Continuous_Scales.png)\n",
        "\n",
        "\n",
        "In simplified terms, the process of training a decision tree and predicting the target features of query instances is as follows:\n",
        "\n",
        "1. Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature\n",
        "\n",
        "2. Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process\n",
        "\n",
        "3. Grow the tree until we accomplish a stopping criteria --> create leaf nodes which represent the predictions we want to make for new query instances\n",
        "\n",
        "4. Show query instances to the tree and run down the tree until we arrive at leaf nodes\n",
        "\n",
        "5. DONE - Congratulations you have found the answers to your questions\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Train_Predict_simplified.png)\n",
        "\n",
        "\n",
        "So what do we know until know?\n",
        "\n",
        "In principal decision trees can be used to predict the target feature of a unknown query instance by building a model based on existing data for which the target feature values are known (supervised learning). Additionally, we know that this model can make predictions for unknown query instances because it models the relationship between the known descriptive features and the know target feature. In our following example, the tree model learns \"how a specific animal species looks like\" respectively the combination of descriptive feature values distinctive for animal species.\n",
        "Additionally, we know that to train a decision tree model we need a dataset consisting of a number of training examples characterized by a number of descriptive features and a target feature.\n",
        "DrawingWhat we do not know until know is: How we can build a tree model. To answer that question we should recapitulate what we try to achieve using a decision tree model. We want, given a dataset, train a model which kind of learns the relationship between the descriptive features and a target feature such that we can present the model a new, unseen set of query instances and predict the target feature values for these query instances. Lets further recapitulate the general shape of a decision tree. We know that we have at the bottom of the tree leaf nodes which contain (in the optimal case) target feature values. To make this more illustrative we use as a practical example a simplified version of the UCI machine learning Zoo Animal Classification dataset which includes properties of animals as descriptive features and the and the animal species as target feature. In our example the animals are classified as being Mammals or Reptiles based on whether they are toothed, have legs and do breath. The dataset looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFMVJ6PnOSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "ec0e321a-8925-49dc-940d-5986603dfbcf"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n",
        "                     \"hair\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
        "                     \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
        "                     \"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\n",
        "                     \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n",
        "                    columns=[\"toothed\",\"hair\",\"breathes\",\"legs\",\"species\"])\n",
        "\n",
        "features = data[[\"toothed\",\"hair\",\"breathes\",\"legs\"]]\n",
        "target = data[\"species\"]\n",
        "\n",
        "data"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toothed</th>\n",
              "      <th>hair</th>\n",
              "      <th>breathes</th>\n",
              "      <th>legs</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Reptile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Reptile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Reptile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mammal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Reptile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  toothed   hair breathes   legs  species\n",
              "0    True   True     True   True   Mammal\n",
              "1    True   True     True   True   Mammal\n",
              "2    True  False     True  False  Reptile\n",
              "3   False   True     True   True   Mammal\n",
              "4    True   True     True   True   Mammal\n",
              "5    True   True     True   True   Mammal\n",
              "6    True  False    False  False  Reptile\n",
              "7    True  False     True  False  Reptile\n",
              "8    True   True     True   True   Mammal\n",
              "9   False  False     True   True  Reptile"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVyVz_Q9pJlj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Hence, to come back to our initial question, each leaf node should (in the best case) only contain \"Mammals\" or \"Reptiles\". The task for us is now to find the best \"way\" to split the dataset such that this can be achieved. What do I mean when I say split? Well consider the dataset above and think about what must be done to split the dataset into a Dataset 1 containing as target feature values (species) only Mammals and a Dataset 2, containing only Reptiles. To achieve that, in this simplified example, we only need the descriptive feature hair since if hair is TRUE, the associated species is always a Mammal. Hence in this case our tree model would look like:\n",
        "\n",
        "![](https://www.python-course.eu/images/Reptile_Mammal_distinction.png)\n",
        "\n",
        "That is, we have split our dataset by asking the question if the animal has hair or not. And exactly this asking and therewith splitting is the key to the decision tree models. Now in that case the splitting has been very easy because we only have a small number of descriptive features and the dataset is completely separable along the values of only one descriptive feature. However, most of the time datasets are not that easily separable and we must split the dataset more than one time (\"ask more than one question\"). Here, the next question directly arises: Given that we have to split the dataset more than one time, that is, ask more then one question to separate the dataset, Which is the descriptive feature we should start with (root node) and in which order should we ask questions (build the interior nodes) that is, use descriptive features to split the dataset on? Well, we have seen that using the hair descriptive feature seems to occupy the most information about the target feature since we only need this feature to perfectly split the dataset. Hence it would be useful to measure the \"informativeness\" of the features and use the feature with the most \"informativeness\" as the feature which should be used to split the data on. From now on, we use the term information gain as a measure of \"informativeness\" of a feature. In the following section we will introduce some mathematical terms and derive how the information gain is calculated as well as how we can build a tree model based on that.\n",
        "\n",
        "The Maths Behind Decision Trees\n",
        "\n",
        "\n",
        "In the preceding section we have introduced the information gain as a measure of how good a descriptive feature is suited to split a dataset on. To be able to calculate the information gain, we have to first introduce the term entropy of a dataset. The entropy of a dataset is used to measure the impurity of a dataset and we will use this kind of informativeness measure in our calculations. There are also other types of measures which can be used to calculate the information gain. The most prominent ones are the: Gini Index, Chi-Square, Information gain ratio, Variance. The term entropy (in information theory) goes back to Claude E. Shannon. The idea behind the entropy is, in simplified terms, the following: Imagine you have a lottery wheel which includes 100 green balls. The set of balls within the lottery wheel can be said to be totally pure because only green balls are included. To express this in the terminology of entropy, this set of balls has a entropy of 0 (we can also say zero impurity). Consider now, 30 of these balls are replaced by red and 20 by blue balls.\n",
        "\n",
        "![](https://www.python-course.eu/images/Shannons_Entropy_Impurity.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you now draw another ball from the lottery wheel, the probability of receiving a green ball has dropped from 1.0 to 0.5. Since the impurity increased, the purity decreased, hence also the entropy increased. Hence we can say, the more \"impure\" a dataset, the higher the entropy and the less \"impure\" a dataset, the lower the entropy. Shannon's entropy model uses the logarithm function (log2(P(x))) to measure the entropy and therewith the impurity of a dataset since the higher the probability of getting a specific result == P(x) (randomly drawing a green ball), the closer approaches the binary logarithm 1.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure()\n",
        "ax=fig.add_subplot(111)\n",
        "\n",
        "ax.plot(np.linspace(0.01,1),np.log2(np.linspace(0.01,1)))\n",
        "ax.set_xlabel(\"P(x)\")\n",
        "ax.set_ylabel(\"log2(P(x))\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Once a dataset contains more than one \"type\" of elements specifically more than one target feature value, the impurity will be greater than zero. Therewith also the entropy of the dataset will be greater than zero. Hence it is useful to sum up the entropies of each possible target feature value and weight it by the probability that we achieve these values assuming we would randomly draw values from the target feature value space (What is the probability to draw a green ball just by chance? Exactly, 0.5 and therewith we have to weight the entropy calculated for the green balls with 0.5). This finally leads to the formal definition of Shannon's entropy which serves as the baseline for the information gain calculation:\n",
        "\n",
        "H(x)=−∑for k ∈target(P(x=k)∗log2(P(x=k)))\n",
        "\n",
        "where we say that P(x=k) is the probability, that the target feature takes a specific value k. Hence applying this formula to our example with the three colored balls we get:\n",
        "\n",
        "Green balls: H(x=green)=0.5∗log2(0.5)=−0.5\n",
        "Blue balls: H(x=blue)=0.2∗log2(0.2)=−0.464\n",
        "Red balls: H(x=red)=0.3∗log2(0.3)=−0.521\n",
        "\n",
        "H(x): H(x)=−((−0.5)+(−0.464)+(−0.521))=1.485\n",
        "Lets apply this approach to our original dataset where we want to predict the animal species. Our dataset has two target feature values in its target feature value space {Mammal, Reptile}. Where P(x=Mammal)=0.6 and P(x=Reptile)=0.4 Hence the entropy of our dataset regarding the target feature is calculated with:\n",
        "\n",
        "H(x)=−((0.6∗log2(0.6))+(0.4∗log2(0.4)))=0.971\n",
        "\n",
        "So where are we now on our way towards creating a tree model?\n",
        "We have now determined the total impurity/purity (≈ entropy) of our dataset which equals to approximately 0.971. Now our task is to find the best feature in terms of information gain (Remember that we want to find the feature which splits the data most accurate along the target feature values) which we should use to first split our data on (which serves as root node). Remember that the hair feature is no longer part of our feature set.\n",
        "Following this, how can we check which of the descriptive features most accurately splits the dataset, that is, remains the dataset with the lowest impurity ≈ entropy or in other words best classifies the target features by its own? Well, we use each descriptive feature and split the dataset along the values of these descriptive feature and then calculate the entropy of the dataset once we have split the data along the feature values. This gives us the remaining entropy after we have split the dataset along the feature values. Next, we subtract this value from the originally calculated entropy of the dataset to see how much this feature splitting reduces the original entropy. The information gain of a feature is calculated with:\n",
        "\n",
        "InfoGain(featured)=Entropy(D)−Entropy(featured)\n",
        "\n",
        "\n",
        "So the only thing we have to do is to split the dataset along the values of each feature and then treat these sub sets as if they were our \"original\" dataset in terms of entropy calculation. The formula for the Information Gain calculation per feature is:\n",
        "\n",
        "InforGain(featured,D)=Entropy(D)−∑t ∈ feature(|featured=t||D|∗H(featured=t))\n",
        "\n",
        "=\n",
        "Entropy(D)−∑t ∈ feature(|featured=t||D|∗(−∑k ∈ target(P(target=k,featured=t)∗log2(P(target=k,featured=t))))\n",
        "\n",
        "\n",
        "Summarized, for each descriptive feature, we sum up the resulting entropies for splitting the dataset along the feature values and additionally weight the feature value entropies by their occurrence probability.\n",
        "\n",
        "![](https://www.python-course.eu/images/Information_Gain_Calculation.png)\n",
        "\n",
        "Now we will calcuate the Information gain for each descriptive feature:\n",
        "\n",
        "toothed:\n",
        "\n",
        "![](https://www.python-course.eu/images/Entropy_Calculation_of_the_toothed_feature.png)\n",
        "\n",
        "= 0.963547\n",
        "\n",
        "InfoGain(toothed)=0.971−0.963547= 0.00745\n",
        "\n",
        "breathes:\n",
        "\n",
        "H(breathes)=(910∗−((69∗log2(69))+(39∗log2(39)))+110∗−((0)+(1∗log2(1)))) = 0.82647\n",
        "\n",
        "InfoGain(breathes)=0.971−0.82647= 0.1445\n",
        "\n",
        "legs:\n",
        "\n",
        "H(legs)=710∗−((67∗log2(67))+(17∗log2(17)))+310∗−((0)+(1∗log2(1)))=0.41417\n",
        "\n",
        "InfoGain(legs)=0.971−0.41417= 0.5568\n",
        "\n",
        "Hence the splitting the dataset along the feature legs results in the largest information gain and we should use this feature for our root node.\n",
        "Hence for the time being the decision tree model looks like:\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Root_Node_Split.png)\n",
        "\n",
        "\n",
        "We see that for legs == False, the target feature values of the remaining dataset are all Reptile and hence we set this as leaf node because we have a pure dataset (Further splitting the dataset on any of the remaining two features would not lead to a different or more accurate result since whatever we do after this point, the prediction will remain Reptile). Additionally, you see that the feature legs is no longer included in the remaining datasets. Because we already has used this (categorical) feature to split the dataset on it must not be further used.\n",
        "\n",
        "Until now we have found the feature for the root node as well as a leaf node for legs == False. The same steps for information gain calculation must now be accomplished also for the remaining dataset for legs == True since here we still have a mixture of different target feature values. Hence:\n",
        "\n",
        "Information gain calculation for the features toothed and breathes for the remaining dataset legs == True:\n",
        "\n",
        "\n",
        "Entropy of the (new) sub data set after first split:\n",
        "\n",
        "H(D)=−((67∗log2(67))+(17∗log2(17)))= 0.5917\n",
        "\n",
        "toothed:\n",
        "\n",
        "H(toothed)=57∗−((1∗log2(1))+(0))+27∗−((12∗log2(12))+(12∗log2(12)))= 0.285\n",
        "\n",
        "InfoGain(toothed)=0.5917−0.285= 0.3067\n",
        "\n",
        "breathes:\n",
        "\n",
        "H(breathes)= 77∗−((67∗log2(67))+(17∗log2(17)))+0= 5917\n",
        "\n",
        "InfoGain(toothed)= 0.5917−0.5917= 0\n",
        "\n",
        "The dataset for toothed == False still contains a mixture of different target feature values why we proceed partitioning on the last left feature (== breathes)\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Decision_Tree_Completed.png)\n",
        "\n",
        "\n",
        "Mind the last split (node) where the dataset got split on the breathes feature. Here the breathes feature solely contains data where breaths == True. Hence for breathes == False there are no instances in the dataset and therewith there is no sub-Dataset which can be built. In that case we return the most frequently occurring target feature value in the original dataset which is Mammal. This is an example how our tree model generalizes behind the training data.\n",
        "If we consider the other branch, that is breathes == True we know, that after splitting the Dataset on the values of a specific feature (breathes {True,False}) in our case, the feature must be removed. Well, that leads to a dataset where no more features are available to further split the dataset on. Hence we stop growing the tree and return the mode value of the direct parent node which is \"Mammal\".\n",
        "\n",
        "\n",
        "That leads us to the introduction of the ID3 algorithm which is a popular algorithm to grow decision trees, published by Ross Quinlan in 1986. Besides the ID3 algorithm there are also other popular algorithms like the C4.5, the C5.0 and the CART algorithm which we will not further consider here. Before we introduce the ID3 algorithm lets quickly come back to the stopping criteria of the above grown tree. We can define a nearly arbitrarily large number of stopping criteria. Assume for instance, we say a tree is allowed to grow for only 2 seconds and then the growing process should stop - Well that would be a stopping criteria - Nonetheless, there are mainly three useful cases in which we stop the tree from growing assuming we do not stop it beforehand by defining for instance a maximum tree depth or a minimum information gain value. We stop the tree from growing when:\n",
        "1. All rows in the target feature have the same value\n",
        "2. The dataset can be no longer split since there are no more features left\n",
        "3. The dataset can no longer be split since there are no more rows left / There is no data left\n",
        "\n",
        "By definition, we say that if the growing gets stopped because of stopping criteria two, the leaf node should predict the most frequently occurring target feature value of the superior (parent) node. If the growing gets stopped because of the third stopping criteria, we assign the leaf node the mode target feature value of the original dataset.\n",
        "\n",
        "\n",
        "Attention, we now introduce the ID3 algorithm:\n",
        "\n",
        "The pseudocode for the ID3 Algorithm is based on the pseudocode illustation of (Mitchell, 1997).\n",
        "\n",
        "ID3(D,Feature_Attributes,Target_Attributes)\n",
        "\n",
        "    Create a root node r\n",
        "\n",
        "    Set r to the mode target feature value in D\n",
        "\n",
        "    If all target feature values are the same:\n",
        "        return r\n",
        "\n",
        "    Else:\n",
        "        pass\n",
        "\n",
        "    If Feature_Attributes is empty:\n",
        "        return r\n",
        "\n",
        "    Else:\n",
        "        Att = Attribute from Feature_Attributes with the largest information gain value\n",
        "        r = Att\n",
        "\n",
        "        For values in Att:\n",
        "            Add a new node below r where node_values = (Att == values)\n",
        "            Sub_D_values = (Att == values)\n",
        "\n",
        "            If Sub_D_values == empty:\n",
        "                Add a leaf node l where l equals the mode target value in D\n",
        "            Else:\n",
        "                add Sub_Tree with ID3(Sub_D_values,Feature_Attributes = Feature_Attributes without Att, Target_Attributes)\n",
        "\n",
        "                Well this pseudocode is probably a little bit confusing if you are new to decision trees and you don't have a mental picture of a decision tree on your mind. Therefore we will illustrate this pseudocode in pictures to make things a little bit more clear -hopefully-.\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/ID3_Pseudocode_illstration.png)\n",
        "\n",
        "Classification Decision trees from scratch with Python\n",
        "Since we now know the principal steps of the ID3 algorithm, we will start create our own decision tree classification model from scratch in Python.\n",
        "\n",
        "Therefore we will use the whole UCI Zoo Data Set.\n",
        "This dataset consists of 101 rows and 17 categorically valued attributes defining whether an animal has a specific property or not (e.g.hairs, feathers,..). The first attribute represents the name of the animal and will be removed. The target feature consist of 7 integer values [1 to 7] which represents [1:Mammal, 2:Bird, 3:Reptile, 4:Fish, 5:Amphibian, 6:Bug, 7:Invertebrate]\n",
        "\n",
        "Though, before we finally start building the decision tree, I want to note a few things:\n",
        "The intention of the following code is not to create a highly efficient and robust implementation of a ID3 decision tree. For this purpose bright heads have created the prepackaged sklearn decision tree model which we will use in the next section.\n",
        "With the following code I want to to provide and show the basic principle and steps behind creating a decision tree from scratch with the goal that we can use the prepackaged modules more efficiently because we understand and know what they are doing and can eventually, build our own machine learning model.\n",
        "That said, there are four important steps:\n",
        "\n",
        "The calculation of the Information Gain\n",
        "The recursive call of the TreeModel\n",
        "The building of the actual tree structure\n",
        "The species prediction of a new unseen animal-instance\n",
        "Here the most critical aspects are the recursive call of the TreeModel, the creation of the tree itself (building the tree structure) as well as the prediction of a unseen query instance (the process of wandering down the tree to predict the class of a unseen query instance).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROciPU7inOOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Make the imports of python packages needed\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "\n",
        "#Import the dataset and define the feature as well as the target datasets / columns#\n",
        "dataset = pd.read_csv('data/zoo.csv',\n",
        "                      names=['animal_name','hair','feathers','eggs','milk',\n",
        "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
        "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals\n",
        "\n",
        "\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "dataset=dataset.drop('animal_name',axis=1)\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "################### \n",
        "    \n",
        "###################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    \n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree,default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    #1.\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "\n",
        "training_data = train_test_split(dataset)[0]\n",
        "testing_data = train_test_split(dataset)[1] \n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
        "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
        "          '2': {'hair': {'0': '2', '1': '1'}},\n",
        "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
        "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
        "          '8': '7',\n",
        "          'legs': 'class_type'}}\n",
        "The prediction accuracy is:  86.36363636363636 %\n",
        "\n",
        "As we can see, the prediction accuracy for the zoo dataset is about 86% which is actually not that bad considering that we don't have done any improvements like for instance defining a minimal split size or a minimal amount of instances per leaf or bagging or boosting, or pruning, etc.\n",
        "\n",
        "\n",
        "Decision Trees using sklearn\n",
        "\n",
        "\n",
        "Even if the above code is suitable and important to convey the concepts of decision trees as well as how to implement a classification tree model \"from scratch\", there is a very powerful decision tree classification model implemented in sklearn sklearn.tree.DecisionTreeClassifier¶. Thanks to this model we can implement a tree model faster, more efficient and also neater as we can do it in just a few lines of code. The steps to use the sklearn classification decision tree follow the principal sklearn API which are:\n",
        "\n",
        "Choose the model you want to use --> the DecisionTreeClassifier\n",
        "Set the model hyperparameters --> E.g. number of minimum samples per leaf\n",
        "Create a feature data set as well as a target array containing the labels for the instances\n",
        "Fit the model to the training data\n",
        "Use the fitted model on unseen data.\n",
        "Thats it! As always, the steps are straight forward."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWYsNTdMlfSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import the DecisionTreeClassifier model.\n",
        "\"\"\"\n",
        "\n",
        "#Import the DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "###########################################################################################################\n",
        "\n",
        "##########################################################################################################\n",
        "\n",
        "\"\"\"\n",
        "Import the Zoo Dataset\n",
        "\"\"\"\n",
        "\n",
        "#Import the dataset \n",
        "dataset = pd.read_csv('data/zoo.csv')\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "dataset=dataset.drop('animal_name',axis=1)\n",
        "\n",
        "###########################################################################################################\n",
        "\n",
        "##########################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Split the data into a training and a testing set\n",
        "\"\"\"\n",
        "\n",
        "train_features = dataset.iloc[:80,:-1]\n",
        "test_features = dataset.iloc[80:,:-1]\n",
        "train_targets = dataset.iloc[:80,-1]\n",
        "test_targets = dataset.iloc[80:,-1]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Train the model\n",
        "\"\"\"\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion = 'entropy').fit(train_features,train_targets)\n",
        "\n",
        "###########################################################################################################\n",
        "\n",
        "##########################################################################################################\n",
        "\n",
        "\"\"\"\n",
        "Predict the classes of new, unseen data\n",
        "\"\"\"\n",
        "prediction = tree.predict(test_features)\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "\n",
        "##########################################################################################################\n",
        "\n",
        "\"\"\"\n",
        "Check the accuracy\n",
        "\"\"\"\n",
        "\n",
        "print(\"The prediction accuracy is: \",tree.score(test_features,test_targets)*100,\"%\")\n",
        "The prediction accuracy is:  80.95238095238095 %\n",
        "\n",
        "Cool isn't it? Well, the accuracy is not that mind blowing but this is more likely due to the composition of the data itself as due to the model. Feel free to try different model parameters to improve the accuracy of the model.\n",
        "\n",
        "Advantges and Disadvantages of Decision Trees\n",
        "Since we now have seen how a decision tree classification model is programmed in Python by hand and and by using a prepackaged sklearn model we will consider the main advantages and disadvantages of decision trees in general, that is not only of classification decision trees.\n",
        "\n",
        "Advantages\n",
        "White box, easy to interpret model\n",
        "No feature normalization needed\n",
        "Tree models can handle both continuous and categorical data (Classification and Regression Trees)\n",
        "Can model nonlinear relationships\n",
        "Can model interactions between the different descriptive features\n",
        "Disadvantages\n",
        "If continuous features are used the tree may become quite large and hence less interpretable\n",
        "Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented\n",
        "Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests\n",
        "Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones. Facilitated: There are in general three cases why we want to grow a leaf node: If there are only pure target feature values in a sub_set --> We return this value; If the sub_dataset is empty --> We return the mode value of the original dataset; If there are no features left in the sub_dataset --> We return the mode value of the parent node. If we have now one target feature value whose frequency tops all other frequencies, it is clear why the outcome may be biased towards this value. We can address this by ensuring that the dataset is relatively balanced in terms of the target feature values\n",
        "If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data\n",
        "Features with many levels may be preferred over features with less levels since for them it is \"more easy\" to split the dataset such that the sub_datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain\n",
        "When we illustrate the target feature splitting process, we see that the tree model kind of categorizes the target feature classes into rectangular regions. Hence the tree model assumes that the underlying data can be split respectively represented by these rectangular regions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkuHOg2jxky7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![](![](https://www.python-course.eu/images/Rectangular_Splitting.png))\n",
        "\n",
        "Further issues and variations\n",
        "\n",
        "The first thing which has not been shown above is how to grow a tree when the descriptive features are not categorically but continuously scaled.\n",
        "This does not change much from the above approach with the large difference that we can use a continuously scaled feature multiple times during the growing of the tree and we have to use the mean or mode of a feature regarding the values of the target feature instead of the single (categorical) feature values --> These can no loner be used since there is now a infinite number of different possible values.\n",
        "\n",
        "The second important variation is when we do no longer have a categorically scaled but continuously scaled target feature. If this is the case we call the tree model a regression tree model instead of a classification tree model. Here as one example we can use the variance of a feature regarding the target feature as splitting criteria instead of the information gain. We then use the feature with the lowest weighted variance as splitting feature.\n",
        "\n",
        "We said above that decision trees are prone to overfitting the training data. We also mentioned that this issue can be addressed using a method called pruning. And it is exactly what it sounds like. We prune the tree. Therefore we start at the leaf nodes and simply check if the accuracy grows if we prune the leafs and replace the parent node of these leafs by a leaf node representing the mode target feature value for this node. Following this procedure we wander up the tree until the pruning will not lead to a higher accuracy or until the pruning does not reduce the accuracy. To make a long story short, if pruning does not reduce the accuracy, prune. Done. We have found the tree which results in the maximum accuracy regarding our testing data set.\n",
        "\n",
        "Drawing\n",
        "\n",
        "Another approach to increase the accuracy of a tree model is to use an ensemble approach. With an ensemble approach we create different models (in this case) trees from the original dataset and let the different models make a majority vote on the test dataset. That is, we predict the target values for the test dataset using each of the created models and then return this target feature value which has been predicted by the majority of the models. The most prominent approaches to create decision tree ensemble models are called bagging and boosting. A variant of a boosting-based decision tree ensemble model is called random forest model which is one of the most powerful machine learning algorithms. Ensemble models can also be created by using different splitting criteria for the single models such as the Gini index as well as the Information gain ratio.\n",
        "\n",
        "We have now seen a lot of variations and different approaches to decision tree models. Though, there is no general guideline on which approach should be used. -There is no free lunch- As often, it depends on... and the only real advice which can be given is that you have to try different models with different hyperparameters to find the best fitting model for a specific problem. Nevertheless, ensemble models such as the random forest algorithm have proven as very powerful models.\n",
        "\n",
        "In the following chapters we will address some of the above mentioned variations to get a deeper understanding of decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA_VDr_yyYv7",
        "colab_type": "text"
      },
      "source": [
        "### Regression Trees\n",
        "\n",
        "\n",
        "\n",
        "In the previous chapter about Classification decision Trees we have introduced the basic concepts underlying decision tree models, how they can be build with Python from scratch as well as using the prepackaged sklearn DecisionTreeClassifier method. We have also introduced advantages and disadvantages of decision tree models as well as important extensions and variations. One disadvantage of Classification decision Trees is that they need a target feature which is categorically scaled like for instance weather = {Sunny, Rainy, Overcast, Thunderstorm}.\n",
        "Here arises a problem: What if we want our tree for instance to predict the price of a house given some target feature attributes like the number of rooms and the location? Here the values of the target feature (prize) are no longer categorically scaled but are continuous - A house can have, theoretically, a infinite number of different prices -\n",
        "\n",
        "Thats where Regression Trees come in. Regression Trees work in principal in the same way as Classification Trees with the large difference that the target feature values can now take on an infinite number of continuously scaled values. Hence the task is now to predict the value of a continuously scaled target feature Y given the values of a set of categorically (or continuously) scaled descriptive features X.\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Categorical_Continuous_Prices.png)\n",
        "\n",
        "As stated above, the principle of building a Regression Tree follows the same approach as the creation of a Classification Tree.\n",
        "We search for the descriptive feature which splits the target feature values most purely, divide the dataset along the values of this descriptive feature and repeat this process for each of the sub datasets until we accomplish a stopping criteria.If we accomplish a stopping criteria, we grow a leaf node.\n",
        "Though, a few things changed.\n",
        "First of all, let us consider the stopping criteria we have introduced in the Classification Tree chapter to grow a leaf node:\n",
        "\n",
        "If the splitting process leads to a empty dataset, return the mode target feature value of the original dataset\n",
        "If the splitting process leads to a dataset where no features are left, return the mode target feature value of the direct parent node\n",
        "If the splitting process leads to a dataset where the target feature values are pure, return this value\n",
        "If we now consider the property of our new continuously scaled target feature we mention that the third stopping criteria can no longer be used since the target feature values can now take on an infinite number of different values. Consequently, it is most likely that we will not find pure target feature values until there is only one instance left in the dataset.\n",
        "To make a long story short, there is in general nothing like pure target feature values.\n",
        "\n",
        "To address this issue, we will introduce an early stopping criteria that returns the average value of the target feature values left in the dataset if the number of instances in the dataset is ≤5.\n",
        "In general, while handling with Regression Trees we will return the average target feature values as prediction at a leaf node.\n",
        "The second change we have to make becomes apparent when we consider the splitting process itself.\n",
        "While working with Classification Trees we used the Information Gain (IG) of a feature as splitting criteria. That is, the feature with the largest IG was used to split the dataset on. Consider the following example where we examine only one descriptive feature, lets say the number of bedrooms, and the costs of the house as target feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZUE7XpHlfPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'Number_of_Bedrooms':[2,2,4,1,3,1,4,2],'Price_of_Sale':[100000,120000,250000,80000,220000,170000,500000,75000]})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1EVzT_Nzvyw",
        "colab_type": "text"
      },
      "source": [
        "Now how would we calculate the entropy of the Number_of_Bedrooms feature?\n",
        "\n",
        "H(Number of Bedrooms)=∑j ∈ Number of Bedrooms∗(|DNumber of Bedrooms=j||D|∗(∑k ∈ Price of Sale∗(−P(k | j)∗log2(P(k | j)))))\n",
        "\n",
        "If we calculate the weighted entropies, we see that for j = 3, we get a weighted entropy of 0. We get this result because there is only one house in the dataset with 3 bedrooms. On the other hand, for j = 2 (occurs three times) we will get a weighted entropy of 0.59436.\n",
        "To make a long story short, since our target feature is continuously scaled, the IGs of the categorically scaled descriptive features are no longer appropriate splitting criteria.\n",
        "Well, we could instead categorize the target feature along its values where for instance housing prices between $0 and $80000 are categorized as low, between $80001 and $150000 as middle and > $150001 as high.\n",
        "What we have done here is converting our regression problem into kind of a classification problem. Though, since we want to be able to make predictions from a infinite number of possible values (regression) this is not what we are looking for.\n",
        "\n",
        "Lets come back to our initial issue: We want to have a splitting criteria which allows us to split the dataset in such a way that when arriving a tree node, the predicted value (we defined the predicted value as the mean target feature value of the instances at this leaf node where we defined the minimum number of 5 instances as early stopping criteria) is closest to the actual value.\n",
        "It turns out that the variance is one of the most commonly used splitting criteria for regression trees where we will use the variance as splitting criteria.\n",
        "The explanation therefore is, that we want to search for the feature attributes which most exactly point to the real target feature values when splitting the dataset along the values of these target features. Therefore, examine the following picture. What do you think which of those two layouts of the Number_of_Bedrooms feature points more exactly to the real sales prize?\n",
        "\n",
        "![](https://www.python-course.eu/images/Concept_of_Variance.png)\n",
        "\n",
        "Well, obviously that one with the smallest variance! We will introduce the maths behind the measure of variance in the next section.\n",
        "For the time being we start by illustrating these by arrows where wide arrows represent a high variance and slim arrows a low variance. We can illustrate that by showing the variance of the target feature for each value of the descriptive feature. As you can see, the feature layout which minimizes the variance of the target feature values when we split the dataset along the values of the descriptive feature is the feature layout which most exactly points to the real value and hence should be used as splitting criteria. During the creation of our Regression Tree model we will use the measure of variance to replace the information gain as splitting criteria.\n",
        "\n",
        "The maths behind regression trees\n",
        "As stated above, the task during growing a Regression Tree is in principle the same as during the creation of Classification Trees. Though, since the IG turned out to be no longer an appropriate splitting criteria (neither is the Gini Index) due to the continuous character of the target feature we must have a new splitting criteria.\n",
        "Therefore we use the variance which we will introduce now.\n",
        "\n",
        "**Variance**\n",
        "\n",
        "Var(x)=∑ni =1(yi−y¯)n−1\n",
        "\n",
        "Where yi are the single target feature values and y¯ is the mean of these target feature values.\n",
        "Taking the example from above the total variance of the *Prize_of_Sale* target feature is calculated with:\n",
        "\n",
        "Var(Price of Sale)=(100000−189375)2+(120000−189375)2+(250000−189375)2+(80000−189375)2+(220000−189375)2+(170000−189375)2+(500000−189375)2+(75000−189375)27\n",
        "=19.903125∗109 #Large Number ;) Though this has no effect on our calculations\n",
        "\n",
        "Since we want to know which descriptive feature is best suited to split the target feature on, we have to calculate the variance for each value of the descriptive feature with respect to the target feature values.\n",
        "Hence for the *Number_of_Rooms* descriptive feature above we get for the single numbers of rooms:\n",
        "\n",
        "Var(Number of Rooms = 1)=(80000−125000)2+(170000−125000)21=4050000000\n",
        "\n",
        "Var(Number of Rooms = 2)=(100000−98333.3)2+(120000−98333.3)2+(75000−98333.3)22=508333333.3\n",
        "\n",
        "Var(Number of Rooms = 3)=(220000−220000)2=0\n",
        "\n",
        "Var(Number of Rooms = 4)=(250000−375000)2+(500000−375000)21=31250000000\n",
        "\n",
        "Since we now want to also address the issue that there are feature values which occur relatively rarely but have a high variance (This could lead to a very high variance for the whole feature just because of one outliner feature value even though the variance of all other feature values may be small) we address this by calculating the weighted variance for each feature value with:\n",
        "\n",
        "WeightVar(Number of Rooms = 1)=28∗4050000000=1012500000\n",
        "\n",
        "WeightVar(Number of Rooms = 2)=28∗508333333.3=190625000\n",
        "\n",
        "WeightVar(Number of Rooms = 3)=28∗0=0\n",
        "\n",
        "WeightVar(Number of Rooms = 4)=28∗31250000000=7812500000\n",
        "\n",
        "Finally, we sum up these weighted variances to make an assessment about the feature as a whole:\n",
        "\n",
        "SumVar(feature)=∑value ∈ featureWeightVar(featurevalue)\n",
        "\n",
        "Which is in our case:\n",
        "\n",
        "1012500000+190625000+0+7812500000=9015625000\n",
        "\n",
        "Putting all this together finally leads to the formula for the weighted feature variance which we will use at each node in the splitting process to determine which feature we should choose to split our dataset on next.\n",
        "\n",
        "feature[choose] =argminf ∈ features ∑l ∈ levels(f)|f=l||f|∗Var(t,f=l)\n",
        "=argminf ∈ features ∑l ∈ levels(f)|f=l||f|∗∑ni = 1(ti−t¯)2n−1\n",
        "\n",
        "Here *f* denotes a single feature, *l* denotes the value of a feature (e.g Price == medium), *t* denotes the value of the target feature in the subset where *f=l*.\n",
        "\n",
        "Following this calculation specification we find the feature at each node to split our dataset on.\n",
        "\n",
        "![](https://www.python-course.eu/images/Splitting_Criteria_Formula.png)\n",
        "\n",
        "## same logic for code as classification.. but instead change to regressor in sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP0x8T7t0xcU",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"rf\"></a>\n",
        "\n",
        "### Random Forest\n",
        "\n",
        "Tree models are known to be high variance, low bias models. In consequence, they are prone to overfit the training data. This is catchy if we recapitulate what a tree model does if we do not prune it or introduce early stopping criteria like a minimum number of instances per leaf node. Well, it tries to split the data along the features until the instances are pure regarding the value of the target feature, there are no data left, or there are no features left to spit the dataset on. If one of the above holds true, we grow a leaf node. The consequence is that the tree model is grown to the maximal depth and therewith tries to reshape the training data as precise as possible which can easily lead to overfitting. Another drawback of classical tree models like the (ID3 or CART) is that they are relatively unstable. This instability can lead to the situation that a small change in the composition of the dataset leads to a completely different tree model.\n",
        "\n",
        "For instance, consider the case where a categorically scaled feature *A* is used as the \"root node feature\". Following, this feature is replaced from the dataset an no longer existent in the sub trees. Now imagine the situation where we replace a single row in the dataset and this change leads to the situation that now feature *B* has the largest information gain or reduction in variance respectively. What does that mean? Well, feature *B* is now preferred over feature *A* as \"root node feature\" which leads to a completely different tree just because we have altered one single instance in the dataset. This situation may not only occur at the root node but also at all interior nodes of the tree.\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/Unstability_of_tree_models.png)\n",
        "\n",
        "The Random Forest approach is based on two concepts, called bagging and subspace sampling. Bagging is the short form for *bootstrap aggregation*. Here we create a multitude of datasets of the same length as the original dataset drawn from the original dataset with replacement (the *bootstrap* in bagging). We then train a tree model for each of the bootstrapped datasets and take the majority prediction of these models for a unseen query instance as our prediction (the *aggregation* in bagging). Here we take the mean or the median for regression tree models and the mode for classification tree models.\n",
        "You may ask why we draw samples with replacement? Well, let us assume our original dataset has 100 instances (rows) and we want to create a Random Forest model consisting of 10 trees where each tree is trained on a dataset of the same length as the original dataset. If we now draw 100 samples from our original dataset without replacement, what will happen? Exactly, nothing since we have figuratively speaking simply shifted the dataset from one container into another. If we do this 10 times and train one tree model on each data set, we will get 10 times the exact same dataset (assuming the same model parameters). If we now predict a unseen query instance and average the outcome of the 10 tree models, that is run the random forest procedure, we have nothing won. This brings us back to our initial question why we use the bagging approach? We use the bagging approach (remember the resampling) because we know that the single tree models are very sensitive to changes in the data and have large variances. To address this issue we create multiple models on differently composed datasets and take the average of their predictions. Here we apply the principle that averaging the variance of multiple models reduces the variance.\n",
        "\n",
        "![](https://www.python-course.eu/images/Variance_of_the_mean.png)\n",
        "\n",
        "\n",
        "The second concept on which the Random Forest approach is based on, is the concept of subspace sampling. Bagging advanced us towards our goal of having a more powerful model creating more accurate results. Unfortunately it turns out that the bagged models are correlated and hence often relatively equal. That is, based on the correlation they produce similar results. This can be reduced to the fact that the bagging is using the whole feature set (all the descriptive features) for each model. Now assume that there are one or two very strong features which surmounts all the others (e.g. the information gain is much much larger for these two features as for the others) in terms of \"predictiveness\". Even if we change the composition of the data by sampling with replacement, these two are likely to remain the dominant features and hence will serve as the root node or the first hidden node layer respectively. In consequence, the trees look all quite similar.\n",
        "\n",
        "![](https://www.python-course.eu/images/Bagging_Subspace_sampling.png)\n",
        "\n",
        "Now it turns out that it is possible that there are structures hidden in data which get lost if we use pure bagging. To evoke these hidden structures we must reinforce other than those two strong features to give them a voice to vote. How can we do that? Well, the simplest approach is to just move those two from our dataset. Obviously this is no good idea since we want to have them but also want to consider and incorporate the voice (votes) of the not so dominant features. To make a long story short, we do this by randomly drawing a number of m⊂p different features for each split in the trees where p denotes the feature space at each split. Here the literature recommends m=p–√.The combination of the bagging and the subspace sampling gives us the desired Random Forest model whose mystery is the assumption that a large number of weak learners are better in terms of prediction accuracy than one strong learner - or why do you think that the \"ask the audience lifeline\" is set as a lifeline in the *who wants to be the millionaire* show?? -. **Done - Congratulations! you now know the concept behind random forest.** which are among the most powerful machine learning algorithms.\n",
        "\n",
        "Random Forests from scratch with Python\n",
        "\n",
        "\n",
        "Luckily for a Random Forest classification model we can use most of the Classification Tree code created in the [Classification Tree](https://www.python-course.eu/Decision_Trees.php) chapter (The same holds true for Random Forest regression models). The only real change we have to implement in the actual tree-building code is that we use at each split a random feature sample of size m=p–√ where p denotes the feature space at this node. All other changes are made \"around\" the tree building code. That is, from a code perspective, the Random Forest is created by giving the tree-code kind of a nice \"super hero suite\".\n",
        "\n",
        "\n",
        "The third change we have to implement is that the Random Forest model actually can not be visualized like a normal tree model and hence the visualization part is obsolete event though, internally each tree is build and we actually could plot each single tree to reason the decision of the Random Forest model. Though, this is not useful when the number of tree models grows to the hundreds or thousands.\n",
        "The forth change is that we have to add a list in which the predictions of the single tree models are stored to finally return the mode value of that list as prediction.\n",
        "Here, only the changes in the tree-building code due to the creation of the Random Forest are commented. For further comments on the tree-building code itself, the reader is referred to the [Classification Tree](https://www.python-course.eu/Decision_Trees.php) chapter. For the sake of visualization we will use the [UCI mushroom dataset](https://archive.ics.uci.edu/ml/datasets/mushroom) here.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i_hVdR3yYD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Make the imports of python packages needed\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import scipy.stats as sps\n",
        "\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('data\\mushroom.csv',header=None)\n",
        "dataset = dataset.sample(frac=1)\n",
        "dataset.columns = ['target','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing',\n",
        "             'gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
        "             'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population',\n",
        "             'habitat']\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "########################################################################################################### \n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "########################################################################################################### \n",
        "###########################################################################################################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"target\"):\n",
        "    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,target_attribute_name=\"target\",parent_node_class = None):\n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    \n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        \n",
        "        ################################################################################################################\n",
        "        ############!!!!!!!!!Implement the subspace sampling. Draw a number of m = sqrt(p) features!!!!!!!!#############\n",
        "        ###############################################################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        features = np.random.choice(features,size=np.int(np.sqrt(len(features))),replace=False)\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "    \n",
        "                \n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "\n",
        "    \n",
        "def predict(query,tree,default = 'p'):\n",
        "        \n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "            result = tree[key][query[key]]\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "\n",
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:round(0.75*len(dataset))].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[round(0.75*len(dataset)):].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "\n",
        "\n",
        "training_data = train_test_split(dataset)[0]\n",
        "testing_data = train_test_split(dataset)[1] \n",
        "\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "#######Train the Random Forest model###########\n",
        "\n",
        "def RandomForest_Train(dataset,number_of_Trees):\n",
        "    #Create a list in which the single forests are stored\n",
        "    random_forest_sub_tree = []\n",
        "    \n",
        "    #Create a number of n models\n",
        "    for i in range(number_of_Trees):\n",
        "        #Create a number of bootstrap sampled datasets from the original dataset \n",
        "        bootstrap_sample = dataset.sample(frac=1,replace=True)\n",
        "        \n",
        "        #Create a training and a testing datset by calling the train_test_split function\n",
        "        bootstrap_training_data = train_test_split(bootstrap_sample)[0]\n",
        "        bootstrap_testing_data = train_test_split(bootstrap_sample)[1] \n",
        "        \n",
        "        \n",
        "        #Grow a tree model for each of the training data\n",
        "        #We implement the subspace sampling in the ID3 algorithm itself. Hence take a look at the ID3 algorithm above!\n",
        "        random_forest_sub_tree.append(ID3(bootstrap_training_data,bootstrap_training_data,bootstrap_training_data.drop(labels=['target'],axis=1).columns))\n",
        "        \n",
        "    return random_forest_sub_tree\n",
        "\n",
        "\n",
        "        \n",
        "random_forest = RandomForest_Train(dataset,50)\n",
        "\n",
        " \n",
        "\n",
        "#######Predict a new query instance###########\n",
        "def RandomForest_Predict(query,random_forest,default='p'):\n",
        "    predictions = []\n",
        "    for tree in random_forest:\n",
        "        predictions.append(predict(query,tree,default))\n",
        "    return sps.mode(predictions)[0][0]\n",
        "\n",
        "\n",
        "query = testing_data.iloc[0,:].drop('target').to_dict()\n",
        "query_target = testing_data.iloc[0,0]\n",
        "print('target: ',query_target)\n",
        "prediction = RandomForest_Predict(query,random_forest)\n",
        "print('prediction: ',prediction)\n",
        "\n",
        "\n",
        "\n",
        "#######Test the model on the testing data and return the accuracy###########\n",
        "def RandomForest_Test(data,random_forest):\n",
        "    data['predictions'] = None\n",
        "    for i in range(len(data)):\n",
        "        query = data.iloc[i,:].drop('target').to_dict()\n",
        "        data.loc[i,'predictions'] = RandomForest_Predict(query,random_forest,default='p')\n",
        "    accuracy = sum(data['predictions'] == data['target'])/len(data)*100\n",
        "    #print('The prediction accuracy is: ',sum(data['predictions'] == data['target'])/len(data)*100,'%')\n",
        "    return accuracy\n",
        "        \n",
        "        \n",
        "        \n",
        "RandomForest_Test(testing_data,random_forest)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "accuracy = []\n",
        "\n",
        "for i in range(1,11,1):\n",
        "    random_forest = RandomForest_Train(dataset,i)\n",
        "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
        "\n",
        "for i in range(10,110,10):\n",
        "    random_forest = RandomForest_Train(dataset,i)\n",
        "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
        "\n",
        "for i in range(100,1100,100):\n",
        "    random_forest = RandomForest_Train(dataset,i)\n",
        "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
        "\n",
        "print(accuracy)\n",
        "ax0.plot(np.logspace(0,3,30),accuracy)\n",
        "ax0.set_yticks(np.linspace(50,100,50))\n",
        "ax0.set_title(\"Accuracy with respect to the numer of trees in the random forest\")\n",
        "ax0.set_xscale('log')\n",
        "ax0.set_xlabel(\"Number of Trees\")\n",
        "ax0.set_ylabel('Accuracy(%)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvqTPs0dyYBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forests using sklearn\n",
        "# we will now use the prepackaged sklearn Random Forest classification model RandomForestClassifier.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "#Encode the feature values which are strings to integers\n",
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
        "\n",
        "\n",
        "X = dataset.drop(['target'],axis=1)\n",
        "Y = dataset['target']\n",
        "\n",
        "\n",
        "#Instantiate the model with 100 trees and entropy as splitting criteria\n",
        "Random_Forest_model = RandomForestClassifier(n_estimators=100,criterion=\"entropy\")\n",
        "\n",
        "\n",
        "#Cross validation\n",
        "accuracy = cross_validate(Random_Forest_model,X,Y,cv=10)['test_score']\n",
        "print('The accuracy is: ',sum(accuracy)/len(accuracy)*100,'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqWD9FC3FTN",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"boo\"></a>\n",
        "\n",
        "### Boosting\n",
        "\n",
        "We will close the tree chapter with an algorithm called *Boosting*. Besides Random Forests, *Boosting* is another powerful approach to increase the predictive power of classical decision and regression tree models. The Boosting algorithm itself can strictly speaking neither learn nor predict anything since it is build kind of on top of some other (weak) algorithm. The Boosting algorithm is called a \"meta algorithm\". The Boosting approach can (as well as the bootstrapping approach), be applied, in principle, to any classification or regression algorithm but it turned out that tree models are especially suited. The accuracy of boosted trees turned out to be equivalent to Random Forests with respect and even often outperforms the latter (see for instance Caruana and Niculescu-Mizil (2008)(*An Empirical Comparison of Supervised Learning Algorithms*)). Hastie et al. (2009) call boosted decision trees the \"best off-the-shelf classifier of the world\" (Hastie et al. 2006 p.340). The mystic behind Boosting is in principal the same as for Random Forest models *-A bunch of weak learners which performs just slightly better than random guessing can be combined to make better predictions than one strong learner-*. Though, the process how these weak learners are created differs.\n",
        "Recapitulate, that during the creation of our Random Forest model we used the concept of Bagging. During Bagging we have grown a number of *M* trees where each was build on a random sample (allowing resampling) of the original dataset where the random sample had the same length as the original dataset but comprises only a randomly drawn subset of the total feature space. After we have created theses models, we let them make a majority vote to make our final decision. The quintessence is that each tree model is created independent from the outcomes of the other tree models. That is, the \"shape\" of the tree model is only influenced by the \"shape\" of the underlying data which in turn is only influenced by chance (*sampling with resampling*). The main difference in the creation of bagged trees using bootstrap aggregation and boosted trees using boosting is that we now replace the (random) resampling by some kind of *weighting* where we allocate the instances with weights and the weights of the *nth* tree depends on the results returned by the previously created (nth−1) tree model. Hence, different from the Random Forest approach where we created an ensemble of tree models in parallel, we now create the ensemble in sequence, where the set up of the actual tree is influenced by the outputs of all the previous tree models by altering the weights of the dataset, the tree model is build on. The point is, that by implementing these weights, we introduce some kind of learning where the creation of the *nth* tree in the boosted model partly depends on the predictions the *nth−1* model has made. Therewith, we replace the more or less \"randomly-guided\" creation of the single datasets during bootstrapping by a \"guided\" creation. The most prominent boosting algorithm is called *AdaBoost* (adaptive boosting) and was developed by Freund and Schapire (1996). The following discussion is based on the AdaBoost Boosting algorithm. The following illustration gives a visual insight into the boosting algorithm.\n",
        "\n",
        "![](https://www.python-course.eu/images/Boosting_visual_approach.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqX3uzt2yX93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Create a Decision Stump\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_validate\n",
        "import scipy.stats as sps\n",
        "\n",
        "\n",
        "# Load in the data and define the column labels\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('data\\mushroom.csv',header=None)\n",
        "dataset = dataset.sample(frac=1)\n",
        "dataset.columns = ['target','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing',\n",
        "             'gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
        "             'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population',\n",
        "             'habitat']\n",
        "\n",
        "\n",
        "\n",
        "# Encode the feature values from strings to integers since the sklearn DecisionTreeClassifier only takes numerical values\n",
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
        "\n",
        "    \n",
        "    \n",
        "Tree_model = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "\n",
        "X = dataset.drop('target',axis=1)\n",
        "Y = dataset['target'].where(dataset['target']==1,-1)\n",
        "\n",
        "\n",
        "\n",
        "predictions = np.mean(cross_validate(Tree_model,X,Y,cv=100)['test_score'])\n",
        "\n",
        "\n",
        "print('The accuracy is: ',predictions*100,'%')\n",
        "\n",
        "# Mind that we have trained and tested the model on the same dataset (the whole dataset) using 100-fold Cross Validation. We get an accuracy of ≈ 73% which is not good but also not that terribly bad considering that we have used a decision stump for classification (split the dataset only once)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7cvuFFi3Ybb",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.python-course.eu/images/Boosting_Decision_Stumps.png)\n",
        "\n",
        "Next, lets see how we can improve this result using a boosted decision stump approach. One thing which might be a bit confusing is that on our way to the final *boosted decision stump*, we use the whole dataset as training and testing dataset (we don't do a train test split). You might remember that we normally want to have a training set, on which we train the model and a testing set on which we test a model - Nevertheless, for Boosting we make an exception and use the whole dataset for training and testing - Just keep this exception in mind-."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChTbJStM3CZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Boosting:\n",
        "\n",
        "    def __init__(self,dataset,T,test_dataset):\n",
        "        self.dataset = dataset\n",
        "        self.T = T\n",
        "        self.test_dataset = test_dataset\n",
        "        self.alphas = None\n",
        "        self.models = None\n",
        "        self.accuracy = []\n",
        "        self.predictions = None\n",
        "    \n",
        "    def fit(self):\n",
        "        # Set the descriptive features and the target feature\n",
        "        X = self.dataset.drop(['target'],axis=1)\n",
        "        Y = self.dataset['target'].where(self.dataset['target']==1,-1)\n",
        "\n",
        "        # Initialize the weights of each sample with wi = 1/N and create a dataframe in which the evaluation is computed\n",
        "        Evaluation = pd.DataFrame(Y.copy())\n",
        "        Evaluation['weights'] = 1/len(self.dataset) # Set the initial weights w = 1/N\n",
        "        \n",
        "\n",
        "        # Run the boosting algorithm by creating T \"weighted models\"\n",
        "        \n",
        "        alphas = [] \n",
        "        models = []\n",
        "        \n",
        "        for t in range(self.T):\n",
        "\n",
        "            # Train the Decision Stump(s)\n",
        "            Tree_model = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1) # Mind the deth one --> Decision Stump\n",
        "            \n",
        "            # We know that we must train our decision stumps on weighted datasets where the weights depend on the results of\n",
        "            # the previous decision stumps. To accomplish that, we use the 'weights' column of the above created \n",
        "            # 'evaluation dataframe' together with the sample_weight parameter of the fit method.\n",
        "            # The documentation for the sample_weights parameter sais: \"[...] If None, then samples are equally weighted.\"\n",
        "            # Consequently, if NOT None, then the samples are NOT equally weighted and therewith we create a WEIGHTED dataset \n",
        "            # which is exactly what we want to have.\n",
        "            model = Tree_model.fit(X,Y,sample_weight=np.array(Evaluation['weights'])) \n",
        "            \n",
        "            # Append the single weak classifiers to a list which is later on used to make the \n",
        "            # weighted decision\n",
        "            models.append(model)\n",
        "            predictions = model.predict(X)\n",
        "            score = model.score(X,Y)\n",
        "\n",
        "            # Add values to the Evaluation DataFrame\n",
        "            Evaluation['predictions'] = predictions\n",
        "            Evaluation['evaluation'] = np.where(Evaluation['predictions'] == Evaluation['target'],1,0)\n",
        "            Evaluation['misclassified'] = np.where(Evaluation['predictions'] != Evaluation['target'],1,0)\n",
        "\n",
        "            # Calculate the misclassification rate and accuracy\n",
        "            accuracy = sum(Evaluation['evaluation'])/len(Evaluation['evaluation'])\n",
        "            misclassification = sum(Evaluation['misclassified'])/len(Evaluation['misclassified'])\n",
        "\n",
        "\n",
        "            # Caclulate the error\n",
        "            err = np.sum(Evaluation['weights']*Evaluation['misclassified'])/np.sum(Evaluation['weights'])\n",
        " \n",
        "   \n",
        "            # Calculate the alpha values\n",
        "            alpha = np.log((1-err)/err)\n",
        "            alphas.append(alpha)\n",
        "\n",
        "\n",
        "            # Update the weights wi --> These updated weights are used in the sample_weight parameter\n",
        "            # for the training of the next decision stump. \n",
        "            Evaluation['weights'] *= np.exp(alpha*Evaluation['misclassified'])\n",
        "\n",
        "            #print('The Accuracy of the {0}. model is : '.format(t+1),accuracy*100,'%')\n",
        "            #print('The missclassification rate is: ',misclassification*100,'%')\n",
        "        \n",
        "        self.alphas = alphas\n",
        "        self.models = models\n",
        "            \n",
        "    def predict(self):\n",
        "        X_test = self.test_dataset.drop(['target'],axis=1).reindex(range(len(self.test_dataset)))\n",
        "        Y_test = self.test_dataset['target'].reindex(range(len(self.test_dataset))).where(self.dataset['target']==1,-1)\n",
        "    \n",
        "        # With each model in the self.model list, make a prediction \n",
        "        \n",
        "        accuracy = []\n",
        "        predictions = []\n",
        "        \n",
        "        for alpha,model in zip(self.alphas,self.models):\n",
        "            prediction = alpha*model.predict(X_test) # We use the predict method for the single decisiontreeclassifier models in the list\n",
        "            predictions.append(prediction)\n",
        "            self.accuracy.append(np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0]))\n",
        "            # The above line of code could be a little bit confusing and hence I will do up the single steps:\n",
        "            # Goal: Create a list of accuracies which can be used to plot the accuracy against the number of base learners used for the model\n",
        "            # 1. np.array(predictions) --> This is the array which contains the predictions of the single models. It has the shape 8124xn\n",
        "            # and hence looks like [[0.998,0.87,...0.87...],[...],[...],[0.99,1.23,...,1.05,0,99...]] \n",
        "            # 2. np.sum(np.array(predictions),axis=0) --> Summs up the first elements of the lists, that is 0,998+...+...+0.99. This is \n",
        "            # done since the formula for the prediction wants us to sum up the predictions of all models for each instance in the dataset. \n",
        "            # Hence if we have for example 3 models than the predictions array has the shape 8124x3 (Imagine a table with 3 columns and\n",
        "            # 8124 rows). Here the first column containst the predictions for the first model, the second column contains the \n",
        "            # prediction for the second model, the third column the prediction for the third model (mind that the\n",
        "            # second and third model are influenced by the results of the first resoectvely the first and the\n",
        "            # second model). This is logical since the results from column (model)\n",
        "            # n-1 are used to alter the weights of the nth model and the results of the nth model are then used to alter the weights\n",
        "            # of the n+1 model. \n",
        "            # 3. np.sign(np.sum(np.array(predictions),axis=0)) --> Since our test target data are elements of {-1,1} and we want to \n",
        "            # have our prediction in the same format, we use the sign function. Hence each column in the accuracy array is like\n",
        "            # [-0.998,1.002,1.24,...,-0.89] and each element represents the combined and weighted prediction of all models up this column\n",
        "            # (so if we are for instance in the 5th column and for the 4th instnace we find the value -0.989, this value represents the \n",
        "            # weighted prediction of a boosted model with 5 base learners for the 4th instance. The 4th instance of the 6th column represents\n",
        "            # the weighted and combined predictions of a boosted model with 6 base learners while the 4th instance of the 4th column represents\n",
        "            # the predction of a model with 4 base learners and so on and so forth...). To make a long story short, we are interested in the \n",
        "            # the sign of these comined predictions. If the sign is positive, we know that the true prediction is more likely postive (1) then\n",
        "            # negaive (-1). The higher the value (postive or negative) the more likely it is that the model returns the correct prediction.\n",
        "            # 4. np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0]) --> With the last step we have transformed the array \n",
        "            # into the shape 8124x1 where the instances are elements {-1,1} and therewith we are now in the situation to compare this \n",
        "            # prediction with our target feature values. The target feature array is of the shape 8124x1 since for each row it contains\n",
        "            # exactly one prediction {-1,1} just as our just created array above --> Ready to compare ;).\n",
        "            # The comparison is done with the == Y_test.values command. As result we get an \n",
        "            # array of the shape 8124x1 where the instances are elements of {True,False} (True if our prediction is consistent with the \n",
        "            # target feature value and False if not). Since we want to calculate a percentage value we have to calculate the fraction of \n",
        "            # instances which have been classified correctly. Therefore we simply sum up the above comparison array \n",
        "            # with the elements {True,False} along the axis 0.\n",
        "            # and divide it by the total number of rows (8124) since True is the same as 1 and False is the same as 0. Hence correct predictions \n",
        "            # increase the sum while false predictions does not change the sum. If we predicted nothing correct the calculation is 0/8124 and \n",
        "            # therewith 0 and if we predicted everything correct, the calculation is 8124/8124 and thereiwth 1. \n",
        "            # 5. self.accuracy.append(np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0])) -->\n",
        "            # After we have computed the above steps, we add the result to the self.accuracy list. This list has the shape n x 1, that is,\n",
        "            # for a model with 5 base learners this list has 5 entries where the 5th entry represents the accuracy of the model when all\n",
        "            # 5 base learners are combined, the 4th element the accuracy of the model when 4 base learners are combined and so on and so forth. This \n",
        "            # procedure has been explained above. That's it and we can plot the accuracy.\n",
        "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n",
        "\n",
        "   \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "######Plot the accuracy of the model against the number of stump-models used##########\n",
        "\n",
        "number_of_base_learners = 50\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "for i in range(number_of_base_learners):\n",
        "    model = Boosting(dataset,i,dataset)\n",
        "    model.fit()\n",
        "    model.predict()\n",
        "\n",
        "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
        "ax0.set_xlabel('# models used for Boosting ')\n",
        "ax0.set_ylabel('accuracy')\n",
        "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1]*100,'%')    \n",
        "                 \n",
        "plt.show()     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4pskxMJ3CWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Boosting using sklearn\n",
        "#As always, we will now use the prepackaged sklearn AdaBoostClassifier with the parameters set to the same values we used above. The documentation says:\n",
        "#\"An AdaBoost [Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting”, 1995] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\"[1]\n",
        "#The parameters we have to adjust are:\n",
        "\n",
        "base_estimator: We set this to the default value which is DecisionTreeClassifier as we have used above. Mind that we could also define a variable like estimator= DecisionTreeClassifier and parametrize this estimator by setting max_depth = 1, criterion = \"entropy\",... But for convenience we will omit this here\n",
        "n_estimators: This is the number of base learners which should be used. We set this to 400 as above.\n",
        "learning_rate: The default value is 1.0 and reduces the contribution of each tree by the learning rate.We set this to the default value since we don't have explicitly incorporated a learning rate.\n",
        "\n",
        "The rest of the parameter is set to the default values.\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
        "    \n",
        "X = dataset.drop(['target'],axis=1)\n",
        "Y = dataset['target']\n",
        "\n",
        "#model = DecisionTreeClassifier(criterion='entropy',max_depth=1)\n",
        "#AdaBoost = AdaBoostClassifier(base_estimator= model,n_estimators=400,learning_rate=1)\n",
        "\n",
        "AdaBoost = AdaBoostClassifier(n_estimators=400,learning_rate=1,algorithm='SAMME')\n",
        "\n",
        "AdaBoost.fit(X,Y)\n",
        "\n",
        "prediction = AdaBoost.score(X,Y)\n",
        "\n",
        "print('The accuracy is: ',prediction*100,'%')\n",
        "The accuracy is:  100.0 %"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfBHByTV39qM",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"pca\"></a>\n",
        "\n",
        "### PCA\n",
        "\n",
        "What is Principal Component Analysis\n",
        "\n",
        "When we perform Principal Component Analysis (PCA) we want to find the principal components of a dataset. Surprising isn't it? Well, what are the principal components of a dataset and why do we want to find them, and what do they tell us? The principal components of a dataset are the \"directions\" in a dataset which hold the most variation (I assume that you have a basic understanding of the term variance. If not, look it up [here](https://www.mathsisfun.com/data/standard-deviation.html)). In simplified terms, the first principal component of a dataset is the direction along the dataset with the highest variation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEEOQ5Jw3CUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"1. Collect the data\"\"\"\n",
        "\n",
        "df = pd.read_table('data\\Wine.txt',sep=',',names=['Alcohol','Malic_acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n",
        "                                                 'Flavanoids','Nonflavanoid_phenols','Proanthocyanins','Color_intensity','Hue',\n",
        "                                                 'OD280/OD315_of_diluted_wines','Proline'])\n",
        "\n",
        "target = df.index\n",
        "\n",
        "\n",
        "\"\"\"2. Normalize the data\"\"\" \n",
        "\n",
        "\n",
        "df = StandardScaler().fit_transform(df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"3. Calculate the covariance matrix\"\"\"        \n",
        "\n",
        "COV = np.cov(df.T) # We have to transpose the data since the documentation of np.cov() sais \n",
        "                   # Each row of `m` represents a variable, and each column a single\n",
        "                   # observation of all those variables \n",
        "\n",
        "\n",
        "\"\"\"4. Find the eigenvalues and eigenvectors of the covariance matrix\"\"\" \n",
        "\n",
        "eigval,eigvec = np.linalg.eig(COV)\n",
        "print(np.cumsum([i*(100/sum(eigval)) for i in eigval])) # As you can see, the first two principal components contain 55% of\n",
        "                                                        # the total variation while the first 8 PC contain 90%\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"5. Use the principal components to transform the data - Reduce the dimensionality of the data\"\"\"\n",
        "\n",
        "# The wine dataset is 13 dimensional and we want to reduce the dimensionality to 2 dimensions \n",
        "# Therefore we use the two eigenvectors with the two largest eigenvalues and use this vectors \n",
        "# to transform the original dataset.\n",
        "# We want to have 2 Dimensions hence the resulting dataset should be a 178x2 matrix.\n",
        "# The original dataset is a 178x13 matrix and hence the \"principal component matrix\" must be of \n",
        "# shape 13*2 where the 2 columns contain the covariance eigenvectors with the two largest eigenvalues\n",
        "\n",
        "PC = eigvec.T[0:2]\n",
        "\n",
        "\n",
        "\n",
        "data_transformed = np.dot(df,PC.T) # We have to transpose PC because it is of the format 2x178\n",
        "\n",
        "\n",
        "# Plot the data\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "ax0.scatter(data_transformed.T[0],data_transformed.T[1])\n",
        "for l,c in zip((np.unique(target)),['red','green','blue']):\n",
        "    ax0.scatter(data_transformed.T[0,target==l],data_transformed.T[1,target==l],c=c,label=l)\n",
        "\n",
        "ax0.legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq9ir22Z3CR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Sklearn\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"1. Collect the data\"\"\"\n",
        "\n",
        "df1 = pd.read_table('data\\Wine.txt',sep=',',names=['Alcohol','Malic_acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n",
        "                                                 'Flavanoids','Nonflavanoid_phenols','Proanthocyanins','Color_intensity','Hue',\n",
        "                                                 'OD280/OD315_of_diluted_wines','Proline'])\n",
        "\n",
        "target1 = df1.index\n",
        "\n",
        "\n",
        "\"\"\"2. Normalize the data\"\"\" \n",
        "\n",
        "\n",
        "df1 = StandardScaler().fit_transform(df)\n",
        "\n",
        "\n",
        "\"\"\"3. Use the PCA and reduce the dimensionality\"\"\"\n",
        "\n",
        "PCA_model = PCA(n_components=2,random_state=42) # We reduce the dimensionality to two dimensions and set the\n",
        "                                                            # random state to 42\n",
        "data_transformed = PCA_model.fit_transform(df1,target)*(-1) # If we omit the -1 we get the exact same result but rotated by 180 degrees --> -1 on the y axis turns to 1.\n",
        "                                                            # This is due to the definition of the vectors. We can define a vector a as [-1,-1] and as [1,1] \n",
        "                                                            # the lines spanned is the same --> remove the *(-1) and you will see\n",
        "\n",
        "# Plot the data\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "ax0.scatter(data_transformed.T[0],data_transformed.T[1])\n",
        "for l,c in zip((np.unique(target)),['red','green','blue']):\n",
        "    ax0.scatter(data_transformed.T[0,target==l],data_transformed.T[1,target==l],c=c,label=l)\n",
        "\n",
        "ax0.legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xYdZkSW3CO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## LDA\n",
        "\n",
        "#What is LDA\n",
        "#(Fishers) Linear Discriminant Analysis (LDA) searches for the projection of a dataset which maximizes the *between class scatter to within class scatter* (SBSW) ratio of this projected dataset. The goal is to project/transform a dataset A using a transformation matrix w such that the ratio of between class scatter to within class scatter of the transformed dataset Y=wT∗A is maximized. Hence our goal is to find the transformation matrix w that accomplishes this. In Fisher's terms:\n",
        "#*\"Find the linear combination Z=aT∗X such that the between class variance is maximized relative to the within class variance.\"*(Hastie, Tibshirani and Friedman, 2008, p.114). Therewith, LDA is like PCA which we have introduced in the last chapter with the difference, that LDA aims to find the projection of maximum separability. But slowly. Consider the following illustration which shows a dataset consisting of three different classes. We now want to have the within and between class scatter of this dataset.\n",
        "\n",
        "LDA with Python from scratch\n",
        "%%time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "from sklearn.model_selection import train_test_split\n",
        "style.use('fivethirtyeight')\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 0. Load in the data and split the descriptive and the target feature\n",
        "df = pd.read_csv('data/Wine.txt',sep=',',names=['target','Alcohol','Malic_acid','Ash','Akcakinity','Magnesium','Total_pheonols','Flavanoids','Nonflavanoids','Proanthocyanins','Color_intensity','Hue','OD280','Proline'])\n",
        "X = df.iloc[:,1:].copy()\n",
        "target = df['target'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,target,test_size=0.3,random_state=0) \n",
        "\n",
        "\n",
        "# 1. Standardize the data\n",
        "for col in X_train.columns:\n",
        "    X_train[col] = StandardScaler().fit_transform(X_train[col].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "\n",
        "# 2. Compute the mean vector mu and the mean vector per class mu_k\n",
        "mu = np.mean(X_train,axis=0).values.reshape(13,1) # Mean vector mu --> Since the data has been standardized, the data means are zero \n",
        "\n",
        "\n",
        "mu_k = []\n",
        "\n",
        "for i,orchid in enumerate(np.unique(df['target'])):\n",
        "    mu_k.append(np.mean(X_train.where(df['target']==orchid),axis=0))\n",
        "mu_k = np.array(mu_k).T\n",
        "\n",
        "\n",
        "# 3. Compute the Scatter within and Scatter between matrices\n",
        "data_SW = []\n",
        "Nc = []\n",
        "for i,orchid in enumerate(np.unique(df['target'])):\n",
        "    a = np.array(X_train.where(df['target']==orchid).dropna().values-mu_k[:,i].reshape(1,13))\n",
        "    data_SW.append(np.dot(a.T,a))\n",
        "    Nc.append(np.sum(df['target']==orchid))\n",
        "SW = np.sum(data_SW,axis=0)\n",
        "\n",
        "SB = np.dot(Nc*np.array(mu_k-mu),np.array(mu_k-mu).T)\n",
        "   \n",
        "# 4. Compute the Eigenvalues and Eigenvectors of SW^-1 SB\n",
        "eigval, eigvec = np.linalg.eig(np.dot(np.linalg.inv(SW),SB))\n",
        "\n",
        "\n",
        "    \n",
        "# 5. Select the two largest eigenvalues \n",
        "eigen_pairs = [[np.abs(eigval[i]),eigvec[:,i]] for i in range(len(eigval))]\n",
        "eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)\n",
        "w = np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real)) # Select two largest\n",
        "\n",
        "\n",
        "# 6. Transform the data with Y=X*w\n",
        "Y = X_train.dot(w)\n",
        "\n",
        "# Plot the data\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "ax0.set_xlim(-3,3)\n",
        "ax0.set_ylim(-4,3)\n",
        "\n",
        "for l,c,m in zip(np.unique(y_train),['r','g','b'],['s','x','o']):\n",
        "    ax0.scatter(Y[0][y_train==l],\n",
        "                Y[1][y_train==l],\n",
        "               c=c, marker=m, label=l,edgecolors='black')\n",
        "ax0.legend(loc='upper right')\n",
        "\n",
        "\n",
        "# Plot the voroni spaces\n",
        "means = []\n",
        "\n",
        "for m,target in zip(['s','x','o'],np.unique(y_train)):\n",
        "    means.append(np.mean(Y[y_train==target],axis=0))\n",
        "    ax0.scatter(np.mean(Y[y_train==target],axis=0)[0],np.mean(Y[y_train==target],axis=0)[1],marker=m,c='black',s=100)\n",
        "   \n",
        "mesh_x, mesh_y = np.meshgrid(np.linspace(-3,3),np.linspace(-4,3)) \n",
        "mesh = []\n",
        "\n",
        "\n",
        "for i in range(len(mesh_x)):\n",
        "    for j in range(len(mesh_x[0])):\n",
        "        date = [mesh_x[i][j],mesh_y[i][j]]\n",
        "        mesh.append((mesh_x[i][j],mesh_y[i][j]))\n",
        "\n",
        "\n",
        "NN = KNeighborsClassifier(n_neighbors=1)\n",
        "NN.fit(means,['r','g','b'])        \n",
        "predictions = NN.predict(np.array(mesh))\n",
        "\n",
        "ax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.3)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rck2dt5A42_s",
        "colab_type": "text"
      },
      "source": [
        "## GMM\n",
        "\n",
        "The Gaussian Mixture Models (GMM) algorithm is an unsupervised learning algorithm since we do not know any values of a target feature. Further, the GMM is categorized into the clustering algorithms, since it can be used to find clusters in the data. Key concepts you should have heard about are:\n",
        "\n",
        "Multivariate Gaussian Distribution\n",
        "Covariance Matrix\n",
        "Mean vector of multivariate data\n",
        "\n",
        "What are Gaussian Mixture models\n",
        "We want to use Gaussian Mixture models to find clusters in a dataset from which we know (or assume to know) the number of clusters enclosed in this dataset, but we do not know where these clusters are as well as how they are shaped. Finding these clusters is the task of GMM and since we don't have any information instead of the number of clusters, the GMM is an unsupervised approach. To accomplish that, we try to fit a mixture of gaussians to our dataset. That is, we try to find a number of gaussian distributions which can be used to describe the shape of our dataset. A critical point for the understanding is that these gaussian shaped clusters must not be circular shaped as for instance in the KNN approach but can have all shapes a multivariate Gaussian distribution can take. That is, a circle can only change in its diameter whilst a GMM model can (because of its covariance matrix) model all ellipsoid shapes as well. See the following illustration for an example in the two dimensional space. Illustrates the difference between a Gaussian Mixture model approach and a KNN approach where the KNN approach can only model circles whilst the GMM is able to model all types of ellipsoidsWhat I have omitted in this illustration is that the position in space of KNN and GMM models is defined by their mean vector. Hence the mean vector gives the space whilst the diameter respectively the covariance matrix defines the shape of KNN and GMM models.\n",
        "\n",
        "\n",
        "So if we consider an arbitrary dataset like the following:\n",
        "\n",
        "Illustrates a GMM dataset\n",
        "How precise can we fit a KNN model to this kind of dataset, if we assume that there are two clusters in the dataset? Well, not so precise since we have overlapping areas where the KNN model is not accurate. This is due to the fact that the KNN clusters are circular shaped whilst the data is of ellipsoid shape. It may even happen that the KNN totally fails as illustrated in the following figure.Illustrates a dataset to which a KNN model is fitted but which is of ellipsoid shape and hence the KNN model failed to model the data\n",
        "If we would fit ellipsoids to the data, as we do with the GMM approach, we would be able to model the dataset well, as illustrated in the following figure.\n",
        "Illustrates a dataset to which a GMM model is fitted and which model the data well\n",
        "Another weak point of KNN in its original form is that each point is allocated to one cluster, that is, each point either belongs to cluster one or two in our example. So assume, we add some more datapoints in between the two clusters in our illustration above. As you can see, we can still assume that there are two clusters, but in the space between the two clusters are some points where it is not totally clear to which cluster they belong. Tackling this dataset with an classical KNN approach would lead to the result, that each datapoint is allocated to cluster one or cluster two respectively and therewith the KNN algorithm would find a hard cut-off border between the two clusters. Though, as you can see, this is probably not correct for all datapoints since we rather would say that for instance datapoint 1 has a probability of 60% to belong to cluster one and a probability of 40% to belong to cluster two. Hence we want to assign probabilities to the datapoints.\n",
        "\n",
        "Illlustrates the same dataset as above but with the difference, that some datapoints are added in the space between the two clusters such that they are no longer clearly allocatable to one cluster.\n",
        "In such a case, a classical KNN approach is rather useless and we need something let's say more flexible or smth. which adds more likelihood to our clustering. Fortunately,the GMM is such a model. Since we do not simply try to model the data with circles but add gaussians to our data this allows us to allocate to each point a likelihood to belong to each of the gaussians. It is clear, and we know, that the closer a datapoint is to one gaussian, the higher is the probability that this point actually belongs to this gaussian and the less is the probability that this point belongs to the other gaussian. Therefore, consider the following illustration where we have added a GMM to the above data and highlighted point 2. This point is much more likely to belong to cluster/gaussian one (C1) than to cluster/gaussian two (C2). Hence, if we would calculate the probability for this point for each cluster we would get smth. like: With a probability of 99% This point belongs to cluster one, and with a probability of 1% to cluster two.\n",
        "Illlustrates the same dataset as above but with the difference, that some datapoints are added in the space between the two clusters such that they are no longer clearly allocatable.\n",
        "So let's quickly summarize and recapitulate in which cases we want to use a GMM over a classical KNN approach. If we have data where we assume that the clusters are not defined by simple circles but by more complex, ellipsoid shapes, we prefer the GMM approach over the KNN approach. Additionally, if we want to have soft cut-off borders and therewith probabilities, that is, if we want to know the probability of a datapoint to belong to each of our clusters, we prefer the GMM over the KNN approach. Hence, if there arise the two buzz words probabilities and non-circular during our model selection discussion, we should strongly check the use of the GMM.\n",
        "\n",
        "So now that we know that we should check the usage of the GMM approach if we want to allocate probabilities to our clusterings or if there are non-circular clusters, we should take a look at how we can build a GMM model. This is derived in the next section of this tutorial. So much for that: We follow a approach called Expectation Maximization (EM).\n",
        "\n",
        "![](https://www.python-course.eu/images/Difference_between_KNN_and_GMM.png)\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/GMM_Dataset.png)\n",
        "\n",
        "\n",
        "![](https://www.python-course.eu/images/KNN_Fails.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vuTbh2oyX7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GMM using sklearn\n",
        "#So now we will create a GMM Model using the prepackaged sklearn.mixture.GaussianMixture method. As we can see, the actual set up of the algorithm, that is the instantiation as well as the calling of the fit() method does take us only one line of code. Cool isn't it?\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# 0. Create dataset\n",
        "X,Y = make_blobs(cluster_std=0.5,random_state=20,n_samples=1000,centers=5)\n",
        "\n",
        "# Stratch dataset to get ellipsoid data\n",
        "X = np.dot(X,np.random.RandomState(0).randn(2,2))\n",
        "\n",
        "x,y = np.meshgrid(np.sort(X[:,0]),np.sort(X[:,1]))\n",
        "XY = np.array([x.flatten(),y.flatten()]).T\n",
        "\n",
        "GMM = GaussianMixture(n_components=5).fit(X) # Instantiate and fit the model\n",
        "print('Converged:',GMM.converged_) # Check if the model has converged\n",
        "means = GMM.means_ \n",
        "covariances = GMM.covariances_\n",
        "\n",
        "\n",
        "# Predict\n",
        "Y = np.array([[0.5],[0.5]])\n",
        "prediction = GMM.predict_proba(Y.T)\n",
        "print(prediction)\n",
        "\n",
        "# Plot   \n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "ax0.scatter(X[:,0],X[:,1])\n",
        "ax0.scatter(Y[0,:],Y[1,:],c='orange',zorder=10,s=100)\n",
        "for m,c in zip(means,covariances):\n",
        "    multi_normal = multivariate_normal(mean=m,cov=c)\n",
        "    ax0.contour(np.sort(X[:,0]),np.sort(X[:,1]),multi_normal.pdf(XY).reshape(len(X),len(X)),colors='black',alpha=0.3)\n",
        "    ax0.scatter(m[0],m[1],c='grey',zorder=10,s=100)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KvtsaH6oTt",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"tf\"></a>\n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "\n",
        "TensorFlow is an open-source software library for machine learning across a range of tasks. It is a symbolic math library, and also used as a system for building and training neural networks to detect and decipher patterns and correlations, analogous to human learning and reasoning. It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief. TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on 9 November 2015.\n",
        "\n",
        "TensorFlow provides a Python API as well as C++, Haskell, Java, Go and Rust APIs.\n",
        "\n",
        "A tensor can be represented as a multidimensional array of numbers. A tensor has its rank and shape, rank is its number of dimensions and shape is the size of each dimension.\n",
        "\n",
        "# a rank 0 tensor, i.e. a scalar with shape ():\n",
        "42                     \n",
        "# a rank 1 tensor, i.e. a vector with shape (3,):\n",
        "[1, 2, 3] \n",
        "\n",
        "# a rank 2 tensor, i.e. a matrix with shape (2, 3):           \n",
        "[[1, 2, 3], [3, 2, 1]] \n",
        "# a rank 3 tensor with shape (2, 2, 2) :\n",
        "[ [[3, 4], [1, 2]], [[3, 5], [8, 9]]] \n",
        "#\n",
        "Output::\n",
        "[[[3, 4], [1, 2]], [[3, 5], [8, 9]]]\n",
        "All data of TensorFlow is represented as tensors. It is the sole data structure:\n",
        "\n",
        "tf.float32, tf.float64, tf.int8, tf.int16, …, tf.int64, tf.uint8, ...\n",
        "\n",
        "\n",
        "Structure of TensorFlow Programs\n",
        "\n",
        "\n",
        "TensorFlow programs consist of two discrete sections:\n",
        "\n",
        "A graph is created in the construction phase.\n",
        "The computational graph is run in the execution phase, which is a session.\n",
        "\n",
        "![](https://www.python-course.eu/images/session_computational_graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_hwJm-OyX4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Computational Graph:\n",
        "\n",
        "c1 = tf.constant(0.034)\n",
        "c2 = tf.constant(1000.0)\n",
        "x = tf.multiply(c1, c1)\n",
        "y = tf.multiply(c1, c2)\n",
        "final_node = tf.add(x, y)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLk9tToxlfN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ad777084-9657-49cb-bb81-2cca5778a95a"
      },
      "source": [
        "# Running the session: ## could be old version\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    result = sess.run(final_node)\n",
        "    print(result, type(result))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ad46bd1413c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1107\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFLS2EZ7lfLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computational Graph:\n",
        "\n",
        "c1 = tf.constant([3.4, 9.1, -1.2, 9], dtype=tf.float64)\n",
        "c2 = tf.constant([3.4, 9.1, -1.2, 9], dtype=tf.float64)\n",
        "x = tf.multiply(c1, c1)\n",
        "y = tf.multiply(c1, c2)\n",
        "final_node = tf.add(x, y)\n",
        "\n",
        "# Running the session:\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    result = sess.run(final_node)\n",
        "    print(result, type(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjQ50FmS7HS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A computational graph is a series of TensorFlow operations arranged into a graph of nodes. Let's build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. Constant nodes take no input.\n",
        "#Printing the nodes does not output a numerical value. We have defined a computational graph but no numerical evaluation has taken place!\n",
        "\n",
        "c1 = tf.constant([3.4, 9.1, -1.2, 9], dtype=tf.float64)\n",
        "c2 = tf.constant([3.4, 9.1, -1.2, 9], dtype=tf.float64)\n",
        "x = tf.multiply(c1, c1)\n",
        "y = tf.multiply(c1, c2)\n",
        "final_node = tf.add(x, y)\n",
        "\n",
        "print(c1)\n",
        "print(x)\n",
        "print(final_node)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gcfonfU7HQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To evaluate the nodes, we have to run the computational graph within a session. A session encapsulates the control and state of the TensorFlow runtime. The following code creates a Session object and then invokes its run method to run enough of the computational graph to evaluate node1 and node2. By running the computational graph in a session as follows. We have to create a session object:\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "#Now, we can evaluate the computational graph by starting the run method of the session object:\n",
        "\n",
        "result = session.run(final_node)\n",
        "print(result)\n",
        "print(type(result))\n",
        "[  23.12  165.62    2.88  162.  ]\n",
        "\n",
        "#Of course, we will have to close the session, when we are finished:\n",
        "\n",
        "session.close()\n",
        "\n",
        "# It is usually a better idea to work with the with statement, as we did in the introductory examples!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYE491cC7HNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Similarity to NumPy\n",
        "# We will rewrite the following program with Numpy.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "session = tf.Session()\n",
        "x = tf.range(12)\n",
        "print(session.run(x))\n",
        "x2 = tf.reshape(tensor=x, \n",
        "                shape=(3, 4))\n",
        "x2 = tf.reduce_sum(x2, reduction_indices=[0])\n",
        "res = session.run(x2)\n",
        "print(res)\n",
        "\n",
        "x3 = tf.eye(5, 5)\n",
        "res = session.run(x3)\n",
        "print(res)\n",
        "\n",
        "# Now a similar Numpy version:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(12)\n",
        "print(x)\n",
        "x2 = x.reshape((3, 4))\n",
        "res = x2.sum(axis=0)\n",
        "print(res)\n",
        "\n",
        "x3 = np.eye(5, 5)\n",
        "print(x3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWJNWLKT74BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoard\n",
        "\n",
        "#TensorFlow provides functions to debug and optimize programs with the help of a visualization tool called TensorBoard.\n",
        "#TensorFlow creates the necessary data during its execution.\n",
        "#The data are stored in trace files.\n",
        "#Tensorboard can be viewed from a browser using http://localhost:6006/\n",
        "#We can run the following example program, and it will create the directory \"output\" We can run now tensorboard: tensorboard --logdir output\n",
        "\n",
        "#which will create a webserver: TensorBoard 0.1.8 at http://marvin:6006 (Press CTRL+C to quit)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "p = tf.constant(0.034)\n",
        "c = tf.constant(1000.0)\n",
        "x = tf.add(c, tf.multiply(p, c))\n",
        "x = tf.add(x, tf.multiply(p, x))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
        "    print(sess.run(x))\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oPSB0O38HqK",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.python-course.eu/images/tensor_board_screenshot.png)\n",
        "\n",
        "![](https://www.python-course.eu/images/computational_graph.png)\n",
        "\n",
        "Placeholders\n",
        "A computational graph can be parameterized to accept external inputs, known as placeholders. The values for placeholders are provided when the graph is run in a session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZppG2ij73_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "c1 = tf.placeholder(tf.float32)\n",
        "c2 = tf.placeholder(tf.float32)\n",
        "\n",
        "x = tf.multiply(c1, c1)\n",
        "y = tf.multiply(c1, c2)\n",
        "final_node = tf.add(x, y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    result = final_node.eval( {c1: 3.8, c2: 47.11})\n",
        "    print(result)\n",
        "    result = final_node.eval( {c1: [3, 5], c2: [1, 3]})\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h9g3t-1738X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another example:\n",
        "\n",
        "import tensorflow  as tf\n",
        "import numpy as np\n",
        "v1 = np.array([3, 4, 5])\n",
        "v2 = np.array([4, 1, 1])\n",
        "c1 = tf.placeholder(tf.float32, shape=(3,))\n",
        "c2 = tf.placeholder(tf.float32, shape=(3,))\n",
        "x = tf.multiply(c1, c1)\n",
        "y = tf.multiply(c1, c2)\n",
        "final_node = tf.add(x, y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    result = final_node.eval( {c1: v1, c2: v2})\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceRw9KFX735y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "placeholder( dtype, shape=None, name=None )\n",
        "\n",
        "Inserts a placeholder for a tensor that will be always fed. It returns a Tensor that may be used as a handle for feeding a value, but not evaluated directly.\n",
        "\n",
        "Important: This tensor will produce an error if evaluated. Its value must be fed using the feed_dict optional argument to\n",
        "\n",
        "Session.run()\n",
        "\n",
        "Tensor.eval()\n",
        "\n",
        "Operation.run()\n",
        "\n",
        "Args:\n",
        "\n",
        "Parameter\tDescription\n",
        "dtype:\tThe type of elements in the tensor to be fed.\n",
        "shape:\tThe shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.\n",
        "name:\tA name for the operation (optional)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWi4E648732b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Variables\n",
        "#Variables are used to add trainable parameters to a graph. They are constructed with a type and initial value. Variables are not initialized when you call tf.Variable. To initialize the variables of a TensorFlow graph, we have to call global_variables_initializer:\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "W = tf.Variable([.5], dtype=tf.float32)\n",
        "b = tf.Variable([-1], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "model = W * x + b\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    print(sess.run(model, {x: [1, 2, 3, 4]}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzqqeBi07HKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Difference Between Variables and Placeholders\n",
        "\n",
        "The difference between tf.Variable and tf.placeholder consists in the time when the values are passed. If you use tf.Variable, you have to provide an initial value when you declare it. With tf.placeholder you don't have to provide an initial value.\n",
        "\n",
        "The value can be specified at run time with the feed_dict argument inside Session.run\n",
        "\n",
        "A placeholder is used for feeding external data into a Tensorflow computation, i.e. from outside of the graph!\n",
        "\n",
        "If you are training a learning algorithm, a placeholder is used for feeding in your training data. This means that the training data is not part of the computational graph. The placeholder behaves similar to the Python \"input\" statement. On the other hand a TensorFlow variable behaves more or less like a Python variable!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOh_gvSI80u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Example:\n",
        "\n",
        "Calculating the loss:\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "W = tf.Variable([.5], dtype=tf.float32)\n",
        "b = tf.Variable([-1], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "model = W * x + b\n",
        "\n",
        "deltas = tf.square(model - y)\n",
        "loss = tf.reduce_sum(deltas)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        " \n",
        "    print(sess.run(loss, {x: [1, 2, 3, 4], y: [1, 1, 1, 1]}))\n",
        "3.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIAaT6hJ80rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reassigning Values to Variables\n",
        "\n",
        "import tensorflow as tf\n",
        "W = tf.Variable([.5], dtype=tf.float32)\n",
        "b = tf.Variable([-1], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "model = W * x + b\n",
        "deltas = tf.square(model - y)\n",
        "loss = tf.reduce_sum(deltas)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        " \n",
        "    print(sess.run(loss, {x: [1, 2, 3, 4], y: [1, 1, 1, 1]}))\n",
        "\n",
        "    W_a = tf.assign(W, [0.])\n",
        "    b_a =  tf.assign(b, [1.])\n",
        "    sess.run( W_a )\n",
        "    sess.run( b_a)  \n",
        "    # sess.run( [W_a, b_a] ) # alternatively in one 'run'\n",
        "   \n",
        "    print(sess.run(loss, {x: [1, 2, 3, 4], y: [1, 1, 1, 1]}))\n",
        "3.5\n",
        "0.0\n",
        "import tensorflow as tf\n",
        "\n",
        "W = tf.Variable([.5], dtype=tf.float32)\n",
        "b = tf.Variable([-1], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "model = W * x + b\n",
        "deltas = tf.square(model - y)\n",
        "loss = tf.reduce_sum(deltas)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for _ in range(1000):\n",
        "        sess.run(train, \n",
        "                  {x: [1, 2, 3, 4], y: [1, 1, 1, 1]})\n",
        "    writer = tf.summary.FileWriter(\"optimizer\", sess.graph)\n",
        "    print(sess.run([W, b]))\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jl_NPGwlGpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "W = tf.Variable([.5], dtype=tf.float32)\n",
        "b = tf.Variable([-1], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "model = W * x + b\n",
        "deltas = tf.square(model - y)\n",
        "loss = tf.reduce_sum(deltas)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for _ in range(1000):\n",
        "        sess.run(train, \n",
        "                  {x: [1, 2, 3, 4], y: [1, 1, 1, 1]})\n",
        "    writer = tf.summary.FileWriter(\"optimizer\", sess.graph)\n",
        "    print(sess.run([W, b]))\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALojbXb9vZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating Data Sets\n",
        "#We will create data sets for a larger example for the GradientDescentOptimizer.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for quantity, suffix in [(1000, \"train\"), (200, \"test\")]:\n",
        "    samples = np.random.multivariate_normal([-2, -2], [[1, 0], [0, 1]], quantity)\n",
        "    plt.plot(samples[:, 0], samples[:, 1], '.', label=\"bad ones \" + suffix)\n",
        "    bad_ones = np.column_stack((np.zeros(quantity), samples))\n",
        "    \n",
        "    samples = np.random.multivariate_normal([1, 1], [[1, 0.5], [0.5, 1]], quantity)\n",
        "    plt.plot(samples[:, 0], samples[:, 1], '.', label=\"good ones \" + suffix)\n",
        "    good_ones = np.column_stack((np.ones(quantity), samples))\n",
        "    \n",
        "    sample = np.row_stack((bad_ones, good_ones))\n",
        "    np.savetxt(\"data/the_good_and_the_bad_ones_\" + suffix + \".txt\", sample, fmt=\"%1d %4.2f %4.2f\")\n",
        "    \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "number_of_samples_per_training_step = 100\n",
        "num_of_epochs = 1  \n",
        "\n",
        "num_labels = 2 # should be automatically determined\n",
        "\n",
        "def evaluation_func(X):\n",
        "    return predicted_class.eval(feed_dict={x:X}) \n",
        "\n",
        "def plot_boundary(X, Y, pred_func):\n",
        "    # determine canvas borders\n",
        "    mins = np.amin(X, 0)    # array with column minimums\n",
        "    mins = mins - 0.1*np.abs(mins)\n",
        "    maxs = np.amax(X, 0)    # array with column maximums\n",
        "    maxs = maxs + 0.1*maxs\n",
        "\n",
        "    xs, ys = np.meshgrid(np.linspace(mins[0], maxs[0], 300), \n",
        "                         np.linspace(mins[1], maxs[1], 300))\n",
        "\n",
        "    # evaluate model using the dense grid\n",
        "    # c_ creates one array with \"points\" from meshgrid:\n",
        "\n",
        "    \n",
        "    Z = pred_func(np.c_[xs.flatten(), ys.flatten()])\n",
        "    # Z is one-dimensional and will be reshaped into 300 x 300:  \n",
        "    Z = Z.reshape(xs.shape) \n",
        "    \n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xs, ys, Z, colors=('c', 'g', 'y', 'b'))\n",
        "    Xn = X[Y[:,1]==1]\n",
        "    plt.plot(Xn[:, 0], Xn[:, 1], \"bo\")\n",
        "    Xn = X[Y[:,1]==0]\n",
        "    plt.plot(Xn[:, 0], Xn[:, 1], \"go\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_data(fname):\n",
        "    data = np.loadtxt(fname)\n",
        "    labels = data[:, :1] #  array([[ 0.], [ 0.], [ 1.], ...]])\n",
        "    labels_one_hot = (np.arange(num_labels) == labels).astype(np.float32)\n",
        "    data = data[:, 1:].astype(np.float32)\n",
        "    return data, labels_one_hot\n",
        "\n",
        "data_train = \"data/the_good_and_the_bad_ones_train.txt\"\n",
        "data_test = \"data/the_good_and_the_bad_ones_test.txt\"\n",
        "train_data, train_labels = get_data(data_train)\n",
        "test_data, test_labels = get_data(data_test)\n",
        "\n",
        "\n",
        "train_size, num_features = train_data.shape\n",
        "\n",
        "x = tf.placeholder(\"float\", shape=[None, num_features])\n",
        "y_ = tf.placeholder(\"float\", shape=[None, num_labels])\n",
        "   \n",
        "Weights = tf.Variable(tf.zeros([num_features, num_labels]))\n",
        "b = tf.Variable(tf.zeros([num_labels]))\n",
        "y = tf.nn.softmax(tf.matmul(x, Weights) + b)\n",
        "\n",
        "# Optimization.\n",
        "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
        "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
        "\n",
        "# For the test data, hold the entire dataset in one constant node.\n",
        "test_data_node = tf.constant(test_data)\n",
        "\n",
        "# Evaluation.\n",
        "predicted_class = tf.argmax(y, 1)\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run all the initializers to prepare the trainable parameters.\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    sess.run(init)\n",
        "        \n",
        "    # Iterate and train.\n",
        "    for step in range(num_of_epochs * train_size // number_of_samples_per_training_step):\n",
        "    \n",
        "        offset = (step * number_of_samples_per_training_step) % train_size\n",
        "\n",
        "        # get a batch of data\n",
        "        batch_data = train_data[offset:(offset + \n",
        "                                        number_of_samples_per_training_step), :]\n",
        "        batch_labels = train_labels[offset:(offset + number_of_samples_per_training_step)]\n",
        "        \n",
        "        # feed data into the model\n",
        "        train_step.run(feed_dict={x: batch_data, y_: batch_labels})\n",
        "\n",
        "    print('\\nBias vector: ', sess.run(b))\n",
        "    print('Weight matrix:\\n', sess.run(Weights))\n",
        "\n",
        "    print(\"\\nApplying model to first data set:\")\n",
        "    first = test_data[:1]\n",
        "    print(first)\n",
        "    print(\"\\nWx + b: \", sess.run(tf.matmul(first, Weights) + b))\n",
        "    # the softmax function, or normalized exponential function, is a generalization of the \n",
        "    # logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values \n",
        "    # to a K-dimensional vector σ(z) of real values in the range [0, 1] that add up to 1. \n",
        "    print(\"softmax(Wx + b): \", sess.run(tf.nn.softmax(tf.matmul(first, Weights) + b)))\n",
        "\n",
        "    print(\"Accuracy on test data: \", accuracy.eval(feed_dict={x: test_data, y_: test_labels}))\n",
        "    print(\"Accuracy on training data: \", accuracy.eval(feed_dict={x: train_data, y_: train_labels}))\n",
        "    \n",
        "    # classify some values:\n",
        "    print(evaluation_func([[-3, 7.3], [-1,8], [0, 0], [1, 0.0], [-1, 0]]))    \n",
        "        \n",
        "    plot_boundary(test_data, test_labels, evaluation_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsGgCK-WlGmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}